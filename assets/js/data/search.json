[ { "title": "Building a Thread-Safe State Tracker Mediator for WSO2", "url": "/posts/TrackStateOfIntegrationsInWSO2MI/", "categories": "WSO2, WSO2 MI", "tags": "WSO2, WSO2MI, Java", "date": "2025-11-07 00:00:00 -0500", "snippet": "One of the challenges when building integration workflows with WSO2 is handling long-running processes that need to prevent parallel executions. Imagine a scenario where you’re processing an order that involves multiple steps: inventory check, payment processing, shipment coordination, and delivery confirmation. You don’t want another request to start processing the same order while the first one is still in progress. There are complex designs you can do to solve this problem, but WSO2 doesn not provide a solution OOB for this.This is where the WSO2 State Tracker Mediator comes in handy. It’s a thread-safe mediator designed to track process status, manage concurrent access, and monitor progress with built-in expiry support.The ProblemIn enterprise integration scenarios, you often encounter these challenges: Duplicate Processing: Multiple requests trigger the same process simultaneously State Uncertainty: No clear way to know if a process is currently running Resource Wastage: Repeated execution of expensive operationsWithout proper state management, your integration workflows can suffer from race conditions, redundant processing, and operational chaos.The Solution: State Tracker MediatorThe WSO2 State Tracker Mediator provides a simple yet powerful way to: Track process execution state with unique identifiers Block parallel executions of the same process Handle process expiry with configurable TTL (Time To Live) Maintain thread safety for concurrent requestsArchitecture OverviewThe mediator operates on three core operations: START_PROCESS - Initiates tracking for a process IS_PROCESS_RUNNING - Checks if a process is currently active STOP_PROCESS - Marks a process as completedCurrently, the mediator uses an in-memory storage mechanism, hence it can only be used in Single instance deployment. However, it’s designed with extensibility in mind, you can easily swap it out for database or registry-backed storage for distributed scenarios.How It WorksBasic Usage1. Starting a Process&lt;property name=\"STATE_TRACKER_OPERATION\" value=\"START_PROCESS\"/&gt;&lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-12345\"/&gt;&lt;property name=\"PROCESS_STATE_EXPIRY_TIME\" value=\"3600\"/&gt; &lt;!-- optional, in seconds --&gt;&lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt;When you start a process, the mediator: Records the process with a unique identifier (e.g., “order-12345”) Captures the current timestamp Sets an optional expiry time for automatic cleanup2. Checking Process Status&lt;property name=\"STATE_TRACKER_OPERATION\" value=\"IS_PROCESS_RUNNING\"/&gt;&lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-12345\"/&gt;&lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt;&lt;!-- Result available in: PROCESS_IS_RUNNING (true/false) --&gt;Before starting a process, check if it’s already running. The mediator sets the PROCESS_IS_RUNNING property to true or false, allowing you to: Route based on execution state Reject duplicate requests Log or audit concurrent attempts3. Stopping a Process&lt;property name=\"STATE_TRACKER_OPERATION\" value=\"STOP_PROCESS\"/&gt;&lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-12345\"/&gt;&lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt;Once processing completes, stop tracking the process to allow future executions or cleanup resources.Output PropertiesThe mediator exposes the following properties for use in your workflow: Property Description STATE_TRACKER_RESULT Operation result and status PROCESS_IS_RUNNING Boolean indicating if process is currently running PROCESS_START_TIMESTAMP Timestamp when the process started (milliseconds) Real-World ExampleExample 1: Processing customer orders with duplicate prevention.&lt;api name=\"OrderProcessingAPI\" context=\"/orders\"&gt; &lt;resource methods=\"POST\" uri-template=\"/process/{orderId}\"&gt; &lt;inSequence&gt; &lt;!-- Check if order is already being processed --&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"IS_PROCESS_RUNNING\"/&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-{uri.var.orderId}\"/&gt; &lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;!-- If already running, reject the duplicate request --&gt; &lt;filter source=\"get-property('PROCESS_IS_RUNNING')\" regex=\"true\"&gt; &lt;then&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt;{\"error\": \"Order is already being processed\"}&lt;/format&gt; &lt;/payloadFactory&gt; &lt;respond/&gt; &lt;/then&gt; &lt;/filter&gt; &lt;!-- Mark process as started --&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"START_PROCESS\"/&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-{uri.var.orderId}\"/&gt; &lt;property name=\"PROCESS_STATE_EXPIRY_TIME\" value=\"7200\"/&gt; &lt;!-- 2 hours --&gt; &lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;!-- Process the order --&gt; &lt;!-- Mark process as completed --&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"STOP_PROCESS\"/&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"order-{uri.var.orderId}\"/&gt; &lt;class name=\"com.ycr.wso2.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;respond/&gt; &lt;/inSequence&gt; &lt;/resource&gt;&lt;/api&gt;Example 2: Bulk Processing and checking statusIn this scenario, we will be processing a bulk load in the background and checking its status.&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;api context=\"/bulk\" name=\"ProcessRecordAPI\" xmlns=\"http://ws.apache.org/ns/synapse\"&gt; &lt;resource methods=\"POST\" uri-template=\"/process\"&gt; &lt;inSequence&gt; &lt;propertyGroup&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"IS_PROCESS_RUNNING\"/&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"BuldProcessID12345\"/&gt; &lt;/propertyGroup&gt; &lt;class name=\"org.nmdp.integration.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;filter regex=\"false\" source=\"boolean($ctx:PROCESS_IS_RUNNING)\"&gt; &lt;then&gt; &lt;propertyGroup&gt; &lt;property name=\"INTG_NAME\" scope=\"default\" type=\"STRING\" value=\"BulkBusinessPartyTPPAPI\"/&gt; &lt;property expression=\"get-property('MessageID')\" name=\"uuid\" scope=\"default\" type=\"STRING\"/&gt; &lt;property value=\"Bulk records been triggered\" name=\"Message\" scope=\"default\" type=\"STRING\"/&gt; &lt;/propertyGroup&gt; &lt;!-- Accept the bulk request and run in the background --&gt; &lt;clone continueParent=\"false\" id=\"CloneTpp\"&gt; &lt;target&gt; &lt;sequence&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt; { \"Message\": { \"ReturnStatus\": \"Processing\", \"ReturnMessage\": \"Bulk request is accepted for processing, you can track the logs using TraceID.\", \"TraceID\": \"$1\" } } &lt;/format&gt; &lt;args&gt; &lt;arg evaluator=\"xml\" expression=\"$ctx:uuid\"/&gt; &lt;/args&gt; &lt;/payloadFactory&gt; &lt;property name=\"HTTP_SC\" value=\"202\" scope=\"axis2\"/&gt; &lt;respond description=\"respondToClient\"/&gt; &lt;/sequence&gt; &lt;/target&gt; &lt;target&gt; &lt;sequence&gt; &lt;sequence key=\"ProcessYourRecords\"/&gt; &lt;/sequence&gt; &lt;/target&gt; &lt;/clone&gt; &lt;/then&gt; &lt;else&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt; { \"Message\": { \"ReturnStatus\": \"Processing\", \"ReturnMessage\": \"Bulk request is still in progress. Please try after sometime.\" } } &lt;/format&gt; &lt;args&gt;&lt;/args&gt; &lt;/payloadFactory&gt; &lt;property name=\"HTTP_SC\" value=\"409\" scope=\"axis2\"/&gt; &lt;respond description=\"respondToClient\"/&gt; &lt;/else&gt; &lt;/filter&gt; &lt;/inSequence&gt; &lt;outSequence&gt; &lt;/outSequence&gt; &lt;faultSequence&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"BuldProcessID12345\"/&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"STOP_PROCESS\"/&gt; &lt;class name=\"org.nmdp.integration.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt; { \"ReturnMessage\": \"Error\" } &lt;/format&gt; &lt;args/&gt; &lt;/payloadFactory&gt; &lt;property name=\"HTTP_SC\" value=\"400\" scope=\"axis2\"/&gt; &lt;respond description=\"respondToClient\"/&gt; &lt;/faultSequence&gt; &lt;/resource&gt; &lt;!-- Resource to monitor status --&gt; &lt;resource methods=\"GET\" uri-template=\"/process_bulk_status\"&gt; &lt;inSequence&gt; &lt;propertyGroup&gt; &lt;property name=\"STATE_TRACKER_OPERATION\" value=\"IS_PROCESS_RUNNING\"/&gt; &lt;property name=\"PROCESS_IDENTIFIER\" value=\"BuldProcessID12345\"/&gt; &lt;/propertyGroup&gt; &lt;class name=\"org.nmdp.integration.mediator.statetracker.StateTrackerMediator\"/&gt; &lt;filter regex=\"false\" source=\"boolean($ctx:PROCESS_IS_RUNNING)\"&gt; &lt;then&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt; { \"Message\": { \"ReturnStatus\": \"No Process running\", \"ReturnMessage\": \"There are no active jobs running currently.\", \"IsRunning\": \"false\" } } &lt;/format&gt; &lt;args&gt;&lt;/args&gt; &lt;/payloadFactory&gt; &lt;property name=\"HTTP_SC\" value=\"200\" scope=\"axis2\"/&gt; &lt;/then&gt; &lt;else&gt; &lt;payloadFactory media-type=\"json\"&gt; &lt;format&gt; { \"Message\": { \"ReturnStatus\": \"Processing\", \"ReturnMessage\": \"Bulk request is still in progress. Please try after sometime.\", \"IsRunning\": \"true\" } } &lt;/format&gt; &lt;args&gt;&lt;/args&gt; &lt;/payloadFactory&gt; &lt;property name=\"HTTP_SC\" value=\"409\" scope=\"axis2\"/&gt; &lt;/else&gt; &lt;/filter&gt; &lt;respond/&gt; &lt;/inSequence&gt; &lt;outSequence/&gt; &lt;faultSequence/&gt; &lt;/resource&gt;&lt;/api&gt;Design ConsiderationsThread SafetyThe mediator is designed to be thread-safe, making it suitable for high-concurrency environments. It uses appropriate synchronization mechanisms to ensure that state updates are atomic and consistent.In-Memory StorageThe current implementation uses in-memory storage, which is: Fast: No I/O overhead Simple: Easy to deploy and use Suitable for: Single server deployments and clustered environments with session replicationFuture ExtensibilityThe architecture is designed for extensibility. You can implement custom storage backends for: Database Storage: For persistence and cross-instance sharing Registry Storage: Leveraging WSO2’s registry for distributed stateBuilding and DeployingPrerequisites Java 11 or higher Maven 3.6+ WSO2 MI version compatible with your mediatorBuilding from Sourcegit clone https://github.com/yasassri/wso2-state-tracker-midiator.gitcd wso2-state-tracker-midiatormvn clean installDeploying Copy the compiled JAR to &lt;WSO2_HOME&gt;/lib/ Restart the WSO2 server Use the mediator in your integration flows as shown aboveLimitationsCurrently, the mediator: Uses in-memory storage (suitable for single instances) but this can be easily extended to use a datastore.ConclusionThe WSO2 State Tracker Mediator is a lightweight yet effective solution for managing long-running processes and preventing duplicate executions in your integration workflows. By providing a simple API for state management, it helps you build more robust, predictable, and maintainable integration solutions.Whether you’re handling order processing, invoice reconciliation, or any other long-running business process, this mediator gives you the control and visibility you need to ensure reliable execution.Have you faced challenges with duplicate processing or long-running workflows? Try the State Tracker Mediator and share your experience in the comments below!Repository: yasassri/wso2-state-tracker-midiatorLicense: Apache License 2.0" }, { "title": "Solving Datadog Java Agent Conflicts in Kubernetes With A Simple C Library", "url": "/posts/OverrridingjavaOptionsInjectedToJavaProcesses/", "categories": "Kubernetes, Datadog, Java", "tags": "Kubernetes, WSO2, Datadog, Java", "date": "2025-06-22 00:00:00 -0400", "snippet": "Recently, I ran into an issue where Datadog’s admission controller was automatically injecting JAVA_TOOL_OPTIONS into our Java app containers, causing startup issues. This Java app container was executing some other Java commands before the actual app, and the injected JAVA_TOOL_OPTIONS was causing issues for these other Java runs. In this post, I’ll walk you through how I solved this with a lightweight shared library approach.The ProblemWhen you deploy applications in K8s clusters with Datadog monitoring enabled, there are different ways to integrate Datadog. In a Java app, you have to install the Java agent in the container and pass the agent configs to the JVM, typically through the JAVA_TOOL_OPTIONS environment variable. If you have enabled the Datadog Admission Controller in your cluster, Datadog will use a Mutating Webhook and mutate your pod to inject Datadog dependencies and configs. The Datadog init containers used to just inject the JAVA_TOOL_OPTIONS, which was available to you in the container runtime. In other words, if you exec into a pod and echo $JAVA_TOOL_OPTIONS, you should see the variable. So if you needed, you had the capability to unset this if you didn’t want it passed to certain operations happening on container startup. But with the latest Datadog versions, they took it to another level, where the Datadog init container now injects a Linux shared library and sets their library in /etc/ld.so.preload to inject this variable, which made it impossible to remove from environment variables using traditional methods. Shared libraries can be either passed through the LD_PRELOAD variable or by setting the library in /etc/ld.so.preload.What are shared libraries, AKA LD_PRELOAD?Before diving in further, let me explain what LD_PRELOAD does. It’s a Linux feature that allows you to load shared libraries before any other libraries when a program starts. This means: Library Loading Order: Libraries specified in LD_PRELOAD are loaded first. Function Overriding: You can override functions from other libraries. Early Execution: Constructor functions in preloaded libraries run before the main application starts.The difference between LD_PRELOAD and /etc/ld.so.preload: LD_PRELOAD: Environment variable, affects only the current process. /etc/ld.so.preload: System-wide file, affects all processes.Solution: Fight Fire with Fire!Instead of fighting with Kubernetes configurations or changing application code, I decided to create my own shared library that automatically unsets the problematic JAVA_TOOL_OPTIONS environment variable. The key was understanding how LD_PRELOAD works and using it to my advantage. Since Datadog was using /etc/ld.so.preload, their library was loading system-wide. But by using LD_PRELOAD, I could ensure my library loads after Datadog’s but before the Java application starts.The SolutionThe implementation is surprisingly simple—just 12 lines of C code that uses the GNU C library’s constructor attribute:#define _GNU_SOURCE#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;__attribute__((constructor))static void unset_java_tool_options(void) { if (getenv(\"JAVA_TOOL_OPTIONS\")) { unsetenv(\"JAVA_TOOL_OPTIONS\"); printf(\"JAVA_TOOL_OPTIONS has been unset.\\n\"); }}The magic happens with the __attribute__((constructor)) directive—this ensures the function runs automatically when the shared library is loaded. Here’s the execution order: System starts the Java process Datadog’s library loads (from /etc/ld.so.preload) and sets JAVA_TOOL_OPTIONS. My library loads (from LD_PRELOAD) and unsets JAVA_TOOL_OPTIONS. Java application starts with the variable unset.This way, I’m essentially running my unset script after Datadog runs its script, but before the Java process is initialized.Building the LibraryIf you just want to use the compiled library, you can locate the source code and a compiled artifact here: https://github.com/yasassri/shared-lib/actions/runs/15891248186When building the library, you need to make sure the library is compatible with the environment. In my case, I needed to compile this for our production environment which runs on x86_64 Linux. Here’s how you can build it:Local Compilation (Linux x86_64)gcc -shared -fPIC -o unset_java_tool_options.so unset_java_tool_options.cCross-platform Compilation with DockerSince I was working on a Mac with Apple Silicon, I used Docker to ensure I got the right architecture. The key option here is --platform linux/amd64. This approach works great for anyone not on Linux:For Mac users:docker run --platform linux/amd64 --rm -v \"$(pwd)\":/src ubuntu:20.04 sh -c \\ \"apt-get update &amp;&amp; apt-get install -y gcc &amp;&amp; cd /src &amp;&amp; \\ gcc -shared -fPIC -o unset_java_tool_options.so unset_java_tool_options.c\"For Linux users:docker run --rm -v \"$(pwd)\":/src ubuntu:20.04 sh -c \\ \"apt-get update &amp;&amp; apt-get install -y gcc &amp;&amp; cd /src &amp;&amp; \\ gcc -shared -fPIC -o unset_java_tool_options.so unset_java_tool_options.c\"I chose this Docker approach because it: Uses Ubuntu 20.04 to match our container base images. Targets x86_64 architecture for production compatibility. Gives me a consistent build environment regardless of my local machine.Using It In ProductionThe key is using LD_PRELOAD to load our library for the specific process:LD_PRELOAD=./unset_java_tool_options.so java -jar your-application.jarThis approach works because: Datadog’s system-wide library sets the variable. Our process-specific library unsets it. The Java application gets a clean environment.Container IntegrationIn our Dockerfile, I integrated it like this:# Copy the shared libraryCOPY shared-libs/unset_java_tool_options.so /opt/app/You can simply COPY this to your Docker base image and use it whenever you need it. Also note: if you set this globally, Datadog will never run, even for the intended processes.Why This Approach Works WellI really like this solution because it’s: Non-invasive: No changes needed to our application code. Automatic: Works transparently at runtime. Lightweight: Minimal overhead with a tiny shared library. Flexible: Can easily enable or disable by changing the LD_PRELOAD variable.Architecture NotesI compiled this specifically for x86_64 architecture because: Our production K8s clusters run on x86_64 nodes. WSO2 containers typically use x86_64 base images. Cross-compilation ensures compatibility regardless of where I’m developing.When I’d Use This SolutionThis approach is perfect when: You need to disable Datadog’s automatic Java agent injection for specific processes within your container. Legacy applications have compatibility issues with the injected agent.Wrapping UpThis simple shared library shows how a well-targeted solution can solve complex environment conflicts in containerized applications. By using the LD_PRELOAD mechanism and constructor functions, I was able to cleanly intercept and modify the runtime environment without touching any application code.If you’re facing similar environment variable conflicts with Java applications, give this approach a try. Drop a comment if you have any thoughts or questions." }, { "title": "Holding Application startup until Kubernetes resources are created", "url": "/posts/StoringTDigestsInDatabaseAndRecostructingItBackInJavascript/", "categories": "Kubernetes", "tags": "Kubernetes, K8S, WSO2", "date": "2024-05-15 00:00:00 -0400", "snippet": "T-Digest is a statistical algorithm used for approximate calculation of quantiles and percentiles from large data sets. It’s particularly useful when you have a vast amount of data and you want to quickly estimate values like the median, quartiles, or any other percentile without having to process the entire dataset. Once the large dataset is processed the data will be added to the TDigest and from the Digest we are able to estimate the quantiles. In some cases, you may want to persist the TDigest so you can use that data at a different point. In this post, we will explore how we can persist the tidest we create with Nodejs in SQLServer and then reconstruct it.Before we go into the solution it’s important to understand Centroids in TDigest. In essence, centroids in T-Digest serve as central values or representatives of quantile ranges within the data distribution. We will be using these Centroids when storing the Digest in the DB.Once we create the Digest in Javascript, the main problem with the TDigest object that we create is, that it’s not serializable, there isn’t an inbuilt mechanism in the TDigest implementation(Javascript version) to serialize the data out of the box. So as a workaround, we will be extracting all the Centroids from the digest and then storing it in the DB which will allow us to reconstruct the Digest from the DB.Okay, let’s go to the solution.Prerequisites.For this example, I’ll be using SQLServer and NodeJS. So these are the only dependencies. Then of course we need the TDigest npm module for this. Assuming you have node setup you can simply use npm to install the dependencies.npm install tdigest mssqlThen let’s create a simple Database and a table to store the data. For this, I’m using the SQLServer Docker image so I can quickly set up a DB locally to do the implementation etc.Let’s start the docker image.docker run -e \"ACCEPT_EULA=Y\" -e \"MSSQL_SA_PASSWORD=Root#Pass\" -p 1433:1433 -d mcr.microsoft.com/mssql/server:2022-latestThen let’s create a simple table to store the data. I have a column named digest with the type varbinary to store the digest as binary data.CREATE TABLE TDigests (\tid INT IDENTITY(1,1) PRIMARY KEY,\tdigest varbinary(MAX) NOT NULL,);Storing the dataNow we need to construct a TDigest and store it in the database.The following code will generate a digest and add 100,000 entries to it.const digest = new TDigest();// Add 100000 random records to the T-Digestfor (let i = 0; i &lt; 100000; i++) { digest.push(getRandomInt(10, 3000));}digest.compress();console.log(\"90th Percentile before saving: \" + digest.percentile(0.9));Once the digest is created we can use the digest.toArray() as in here to extract all the centroids. Once the centroid array is retrieved you can use the JSON.stringify() method to convert the Javascript object to a JSON object, which will allow us to store the data as a JSON object in the database.Now let’s create a prepared statement and insert the data into the Database.const ps = new sql.PreparedStatement(pool);ps.input('digest', sql.NVarChar); // When we are storing the data we will compress itawait ps.prepare(`INSERT INTO ${tableName} VALUES (COMPRESS(@digest))`);await ps.execute({ 'digest': JSON.stringify(digest.toArray()) });In the above insert statement note the COMPRESS() SQLServer function which allows us to convert the JSON string into binary content before storing it in the DB. This will allow us to save a considerable amount of space in the DB. Also, compression is not mandatory. You can do the compression at the code level as well if that’s what you prefer.Retreiving the dataOnce you insert the data you can retrieve the data using the following code.const getDigestQuery = `SELECT CAST(DECOMPRESS(digest) AS NVARCHAR(MAX)) AS digest FROM ${tableName}`;const result = await pool.query(getDigestQuery)Note when reading the digest we have to decompress it and convert it to a String so we can pass it back to a JSON. Once that is done we can use the following code the recontruct the digest back. For that, we can use digest.push_centroid() method as in here.result.recordset.forEach(element =&gt; { let newDigest = new TDigest(); newDigest.push_centroid(JSON.parse(element.digest)); console.log(\"90th Percentile after constructing: \" + newDigest.percentile(0.9)); });So that’s it. The complete code is something like below.const sql = require('mssql');const { TDigest } = require('tdigest');async function main() { SQL_CONFIG.user = \"SA\"; SQL_CONFIG.password = \"Root#Pass\"; SQL_CONFIG.server = \"localhost\"; SQL_CONFIG.port = 1433; SQL_CONFIG.database = 'master' const tableName = \"TDigests\"; let pool = await getDBPool(); // Initialize a T-Digest data structure const digest = new TDigest(); // Add 100000 random records to the T-Digest for (let i = 0; i &lt; 100000; i++) { digest.push(getRandomInt(10, 3000)); } digest.compress(); console.log(\"90th Percentile before saving: \" + digest.percentile(0.9)); const ps = new sql.PreparedStatement(pool); ps.input('digest', sql.NVarChar); // When we are storing the data we will compress it await ps.prepare(`INSERT INTO ${tableName} VALUES (COMPRESS(@digest))`); await ps.execute({ 'digest': JSON.stringify(digest.toArray()) }); console.log(\"Ok we are done saving the digest, Now let's read and construct it back\") const getDigestQuery = `SELECT CAST(DECOMPRESS(digest) AS NVARCHAR(MAX)) AS digest FROM ${tableName}`; const result = await pool.query(getDigestQuery); result.recordset.forEach(element =&gt; { let newDigest = new TDigest() newDigest.push_centroid(JSON.parse(element.digest)); console.log(\"90th Percentile after constructing: \" + newDigest.percentile(0.9)); }); ps.unprepare(); pool.close();}// Function to generate a random integer between min and max (inclusive)function getRandomInt(min, max) { return Math.floor(Math.random() * (max - min + 1)) + min;}const SQL_CONFIG = { user: '', password: '', database: '', server: '', port: 1433, pool: { max: 10, min: 0, idleTimeoutMillis: 60000 }, options: { encrypt: false, trustServerCertificate: false, trustedConnection: false, } } function getDBPool() { const poolPromise = new sql.ConnectionPool(SQL_CONFIG) .connect() .then(pool =&gt; { console.log('Connected to MSSQL'); return pool }) return poolPromise; }if (require.main === module) { main();}You can find the full code at Github as well.Hope the above helps someone. Please drop a comment if you have any questions." }, { "title": "Delaying Application Startup Until Kubernetes Resources are Created", "url": "/posts/Delaying-Container-startup-until-a-Kubernetes-resources-are-created/", "categories": "Kubernetes, WSO2", "tags": "Kubernetes, WSO2, Clustering", "date": "2024-05-15 00:00:00 -0400", "snippet": "In this post, I will take you through the process I followed when trying to debug an issue that was occurring in one of the applications deployed in K8S. If you are just interested in the part mentioned in the title, skip to the second part of the blog.Why was the application delay needed; Debugging the issue.This was a unique problem I had to work on recently. It all started when WSO2 Enterprise Integrator, which is an ESB, started throwing the following exception. This application had been running for years without an issue, and suddenly the exception occurred. Weird!ERROR {org.wso2.carbon.membership.scheme.kubernetes.KubernetesMembershipScheme} - Kubernetes membership initialization failedorg.wso2.carbon.membership.scheme.kubernetes.exceptions.KubernetesMembershipSchemeException: No member ips found, unable to initialize the Kubernetes membership scheme at org.wso2.carbon.membership.scheme.kubernetes.KubernetesMembershipScheme.init(KubernetesMembershipScheme.java:98) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.initiateCustomMembershipScheme(HazelcastClusteringAgent.java:623) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.configureMembershipScheme(HazelcastClusteringAgent.java:604) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.createConfigForAxis2Mode(HazelcastClusteringAgent.java:402) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.loadHazelcastConfig(HazelcastClusteringAgent.java:448) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.init(HazelcastClusteringAgent.java:175) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.enableClustering(StartupFinalizerServiceComponent.java:293) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.completeInitialization(StartupFinalizerServiceComponent.java:187) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.serviceChanged(StartupFinalizerServiceComponent.java:317) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451) at org.wso2.carbon.throttling.agent.internal.ThrottlingAgentServiceComponent.registerThrottlingAgent(ThrottlingAgentServiceComponent.java:123) at org.wso2.carbon.throttling.agent.internal.ThrottlingAgentServiceComponent.activate(ThrottlingAgentServiceComponent.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197) at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343) at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451) at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:515) at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:220) at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:105) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197) at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343) at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81) at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60) at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40) at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38) at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1230) at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1174) at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1066) at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5433) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5731) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:145) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1707) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1697) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)[localhost-startStop-1] ERROR {org.wso2.carbon.core.internal.StartupFinalizerServiceComponent} - Cannot initialize clusterorg.apache.axis2.clustering.ClusteringFault: Kubernetes membership initialization failed at org.wso2.carbon.membership.scheme.kubernetes.KubernetesMembershipScheme.init(KubernetesMembershipScheme.java:111) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.initiateCustomMembershipScheme(HazelcastClusteringAgent.java:623) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.configureMembershipScheme(HazelcastClusteringAgent.java:604) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.createConfigForAxis2Mode(HazelcastClusteringAgent.java:402) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.loadHazelcastConfig(HazelcastClusteringAgent.java:448) at org.wso2.carbon.core.clustering.hazelcast.HazelcastClusteringAgent.init(HazelcastClusteringAgent.java:175) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.enableClustering(StartupFinalizerServiceComponent.java:293) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.completeInitialization(StartupFinalizerServiceComponent.java:187) at org.wso2.carbon.core.internal.StartupFinalizerServiceComponent.serviceChanged(StartupFinalizerServiceComponent.java:317) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451) at org.wso2.carbon.throttling.agent.internal.ThrottlingAgentServiceComponent.registerThrottlingAgent(ThrottlingAgentServiceComponent.java:123) at org.wso2.carbon.throttling.agent.internal.ThrottlingAgentServiceComponent.activate(ThrottlingAgentServiceComponent.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197) at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343) at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451) at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:515) at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:220) at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:105) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146) at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620) at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197) at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343) at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222) at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861) at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230) at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130) at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214) at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433) at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81) at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60) at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40) at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38) at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1230) at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1174) at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1066) at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5433) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5731) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:145) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1707) at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1697) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.wso2.carbon.membership.scheme.kubernetes.exceptions.KubernetesMembershipSchemeException: No member ips found, unable to initialize the Kubernetes membership scheme at org.wso2.carbon.membership.scheme.kubernetes.KubernetesMembershipScheme.init(KubernetesMembershipScheme.java:98) ... 80 moreThe initial thought was that it had been working fine for three years and suddenly broke, so obviously, the Ops team must have made some changes to the K8S cluster, right? :) After checking with them, it turned out they hadn’t made any changes recently. So, we had to find out what was going on. Let’s look at the exception closely: it’s thrown from the clustering component of WSO2. When you want WSO2 tasks to be coordinated in a multinode WSO2 application setup, you can enable clustering in WSO2. For clustering to work, WSO2 should be able to discover the application nodes and communicate with each other. WSO2 clustering uses Hazelcast underneath with a custom membership schema (membership schema is where you tell Hazelcast how to discover other members of the cluster).So, let’s put on the developer hat. According to the stack trace, the exception is thrown from the org.wso2.carbon.membership.scheme.kubernetes.KubernetesMembershipScheme class. Since WSO2 is an open-source product, we can look into the code and see what exactly is happening. I went through the code to understand what’s happening under the hood in WSO2 when clustering is configured. I won’t go into details, but basically, WSO2 is calling K8S APIs to retrieve member details. If anyone is interested, here is where the exception is thrown: Membership Schema Source.Let’s see how clustering works in WSO2, so we can better understand the issue. Basically, a pod is exposed via a K8S service. When a and pods are created, K8S control plane will create Endpoints to represent each pod, which are grouped based on the selectors. So, in order to identify the members and their IPs, WSO2 calls the Endpoints API in K8S , which returns a list of endpoints registered and their states. The response will be something like below. Name: \"mysvc\", Subsets: [ { Addresses: [{\"ip\": \"10.10.1.1\"}, {\"ip\": \"10.10.2.2\"}], Ports: [{\"name\": \"a\", \"port\": 8675}, {\"name\": \"b\", \"port\": 309}] }, { Addresses: [{\"ip\": \"10.10.3.3\"}], Ports: [{\"name\": \"a\", \"port\": 93}, {\"name\": \"b\", \"port\": 76}] },]From the above response, WSO2 will get all the endpoint IPs and register them as cluster members. However, for some reason, the endpoints are not being returned by the API call.Long story short, after debugging, this is what was happening. For K8S to register endpoints, the pod should be in the Running state. For that to happen, all the containers within the pod should be successfully pulled and started. What was happening is that we had a Fluentbit sidecar container associated with the main container for logging. For some reason, the Fluentbit image was taking more time than the main container to be pulled. So, the main container gets pulled and starts, but the sidecar takes more time to get pulled. Hence, when WSO2 calls the K8S API to get the endpoints, they are not yet registered because the pod is not in the Running state. Later on, we learned that a new corporate proxy was added, which caused the slowness in the image pull process.Ideally, WSO2 should have added some logic to retry if the endpoints were not registered, but they haven’t, so we decided to implement a workaround for the issue. Now let’s get to the solution, the title of the blog :)Delaying Application StartupLet’s see how we can delay the application startup until a certain condition is met. In this case, I will check if the K8S endpoints are registered before starting the application. There are two issues associated with this. First, we need to delay the application startup. Then, we need to make sure health check probes are not failing because the waiting time will be dynamic.Delaying the App StartupTo do this, we can simply add some logic to the init script. For example, something like below:#!/bin/bashecho \"Starting the entrypoint\"# Endpoint API URLKUBE_API_URL=https://${KUBERNETES_SERVICE_HOST}:443/api/v1/namespaces/${KUBERNETES_NAMESPACE}/endpoints/${KUBERNETES_SERVICES}# Bearer token file pathBEARER_TOKEN_FILE=\"/var/run/secrets/kubernetes.io/serviceaccount/token\"# Number of retry attempts.RETRY_COUNT=24# Sleep time between retries (in seconds)RETRY_INTERVAL=5# Function to read Bearer token from fileget_bearer_token() { if [ -f \"$BEARER_TOKEN_FILE\" ]; then cat \"$BEARER_TOKEN_FILE\" else echo \"Error: Bearer token file not found at $BEARER_TOKEN_FILE\" exit 1 fi}# Function to make the API call and check for \"Subsets\" in the JSON response# This is needed for EI clustering.check_api() { bearer_token=$(get_bearer_token) response=$(wget --no-check-certificate --header=\"Authorization: Bearer $bearer_token\" -qO- $KUBE_API_URL) if [[ $response == *\"notReadyAddresses\"* || $response == *\"addresses\"* ]]; then echo \"Subset section found in the response!\" return 0 else echo \"Subset section not found. Retrying...\" return 1 fi}echo \"Checking the endpoint API ${KUBE_API_URL}\"# Retry loopattempt=1while [ $attempt -le $RETRY_COUNT ]; do check_api exit_code=$? if [ $exit_code -eq 0 ]; then # Adding a flag so we can use this for the startup probe touch /home/wso2carbon/check-done # Cleaning the history history -c # Exec wso2 entrypoint echo \"Starting the application\" exec /home/wso2carbon/docker-entrypoint.sh fi echo \"API call failed attempt: ${attempt} and sleeping for ${RETRY_INTERVAL}\" sleep $RETRY_INTERVAL ((attempt++))done# If we reach here, all retrty attempts failed.echo \"Failed after $RETRY_COUNT attempts. Subset section not found.\"exit 1As you can see in the script, I will be calling the Endpoints API to check if the endpoints have been registered: https://${KUBERNETES_SERVICE_HOST}:443/api/v1/namespaces/${KUBERNETES_NAMESPACE}/endpoints/${KUBERNETES_SERVICES}. Also, note that I have already created a service account with the necessary permissions to call the K8S API, and the token for this is mounted to /var/run/secrets/kubernetes.io/serviceaccount/token. As soon as the condition is met, we will create a file indicating the process was completed and start the application.Adjusting the ProbesSince the waiting time is dynamic, we have to make sure the readinessProbe is not started until the application is fully started. Otherwise, as soon as the container starts running, the readinessProbe will kick in and fail after the defined retries. Adding an initial delay is not a good solution as the wait time can vary. So as a solution, we can add a startupProbe. The startupProbe ensures the other health probes are not triggered until the startupProbe is successful. If you look at the above script, we are creating a file named /home/wso2carbon/check-done when the waiting is completed. In the startupProbe, you can simply check the availability of this file. The probes will look like below. startupProbe: initialDelaySeconds: 15 periodSeconds: 5 failureThreshold: 36 successThreshold: 1 exec: command: - /bin/sh - -c - test -f /home/wso2carbon/check-donereadinessProbe: initialDelaySeconds: 20 periodSeconds: 15 failureThreshold: 1 successThreshold: 1 tcpSocket: port: Note: _If you are using this solution for WSO2 applications, the WSO2 app will create a process ID file when the application starts, so you don’t have to create additional files. You can simply check the availability of the /home/wso2carbon//wso2carbon.pid file._So that’s basically it. I hope this will be helpful to someone. Drop a comment if you have any questions." }, { "title": "Storing TDigests In A Database And Reconstructing It Back In Javascript", "url": "/posts/StoringTDigestsInDatabaseAndRecostructingItBackInJavascript/", "categories": "Programming", "tags": "TDigest, Node, Javascript, Algorithms, SQLServer", "date": "2023-09-15 00:00:00 -0400", "snippet": "T-Digest is a statistical algorithm used for approximate calculation of quantiles and percentiles from large data sets. It’s particularly useful when you have a vast amount of data and you want to quickly estimate values like the median, quartiles, or any other percentile without having to process the entire dataset. Once the large dataset is processed the data will be added to the TDigest and from the Digest we are able to estimate the quantiles. In some cases, you may want to persist the TDigest so you can use that data at a different point. In this post, we will explore how we can persist the tidest we create with Nodejs in SQLServer and then reconstruct it.Before we go into the solution it’s important to understand Centroids in TDigest. In essence, centroids in T-Digest serve as central values or representatives of quantile ranges within the data distribution. We will be using these Centroids when storing the Digest in the DB.Once we create the Digest in Javascript, the main problem with the TDigest object that we create is, that it’s not serializable, there isn’t an inbuilt mechanism in the TDigest implementation(Javascript version) to serialize the data out of the box. So as a workaround, we will be extracting all the Centroids from the digest and then storing it in the DB which will allow us to reconstruct the Digest from the DB.Okay, let’s go to the solution.Prerequisites.For this example, I’ll be using SQLServer and NodeJS. So these are the only dependencies. Then of course we need the TDigest npm module for this. Assuming you have node setup you can simply use npm to install the dependencies.npm install tdigest mssqlThen let’s create a simple Database and a table to store the data. For this, I’m using the SQLServer Docker image so I can quickly set up a DB locally to do the implementation etc.Let’s start the docker image.docker run -e \"ACCEPT_EULA=Y\" -e \"MSSQL_SA_PASSWORD=Root#Pass\" -p 1433:1433 -d mcr.microsoft.com/mssql/server:2022-latestThen let’s create a simple table to store the data. I have a column named digest with the type varbinary to store the digest as binary data.CREATE TABLE TDigests (\tid INT IDENTITY(1,1) PRIMARY KEY,\tdigest varbinary(MAX) NOT NULL,);Storing the dataNow we need to construct a TDigest and store it in the database.The following code will generate a digest and add 100,000 entries to it.const digest = new TDigest();// Add 100000 random records to the T-Digestfor (let i = 0; i &lt; 100000; i++) { digest.push(getRandomInt(10, 3000));}digest.compress();console.log(\"90th Percentile before saving: \" + digest.percentile(0.9));Once the digest is created we can use the digest.toArray() as in here to extract all the centroids. Once the centroid array is retrieved you can use the JSON.stringify() method to convert the Javascript object to a JSON object, which will allow us to store the data as a JSON object in the database.Now let’s create a prepared statement and insert the data into the Database.const ps = new sql.PreparedStatement(pool);ps.input('digest', sql.NVarChar); // When we are storing the data we will compress itawait ps.prepare(`INSERT INTO ${tableName} VALUES (COMPRESS(@digest))`);await ps.execute({ 'digest': JSON.stringify(digest.toArray()) });In the above insert statement note the COMPRESS() SQLServer function which allows us to convert the JSON string into binary content before storing it in the DB. This will allow us to save a considerable amount of space in the DB. Also, compression is not mandatory. You can do the compression at the code level as well if that’s what you prefer.Retreiving the dataOnce you insert the data you can retrieve the data using the following code.const getDigestQuery = `SELECT CAST(DECOMPRESS(digest) AS NVARCHAR(MAX)) AS digest FROM ${tableName}`;const result = await pool.query(getDigestQuery)Note when reading the digest we have to decompress it and convert it to a String so we can pass it back to a JSON. Once that is done we can use the following code the recontruct the digest back. For that, we can use digest.push_centroid() method as in here.result.recordset.forEach(element =&gt; { let newDigest = new TDigest(); newDigest.push_centroid(JSON.parse(element.digest)); console.log(\"90th Percentile after constructing: \" + newDigest.percentile(0.9)); });So that’s it. The complete code is something like below.const sql = require('mssql');const { TDigest } = require('tdigest');async function main() { SQL_CONFIG.user = \"SA\"; SQL_CONFIG.password = \"Root#Pass\"; SQL_CONFIG.server = \"localhost\"; SQL_CONFIG.port = 1433; SQL_CONFIG.database = 'master' const tableName = \"TDigests\"; let pool = await getDBPool(); // Initialize a T-Digest data structure const digest = new TDigest(); // Add 100000 random records to the T-Digest for (let i = 0; i &lt; 100000; i++) { digest.push(getRandomInt(10, 3000)); } digest.compress(); console.log(\"90th Percentile before saving: \" + digest.percentile(0.9)); const ps = new sql.PreparedStatement(pool); ps.input('digest', sql.NVarChar); // When we are storing the data we will compress it await ps.prepare(`INSERT INTO ${tableName} VALUES (COMPRESS(@digest))`); await ps.execute({ 'digest': JSON.stringify(digest.toArray()) }); console.log(\"Ok we are done saving the digest, Now let's read and construct it back\") const getDigestQuery = `SELECT CAST(DECOMPRESS(digest) AS NVARCHAR(MAX)) AS digest FROM ${tableName}`; const result = await pool.query(getDigestQuery); result.recordset.forEach(element =&gt; { let newDigest = new TDigest() newDigest.push_centroid(JSON.parse(element.digest)); console.log(\"90th Percentile after constructing: \" + newDigest.percentile(0.9)); }); ps.unprepare(); pool.close();}// Function to generate a random integer between min and max (inclusive)function getRandomInt(min, max) { return Math.floor(Math.random() * (max - min + 1)) + min;}const SQL_CONFIG = { user: '', password: '', database: '', server: '', port: 1433, pool: { max: 10, min: 0, idleTimeoutMillis: 60000 }, options: { encrypt: false, trustServerCertificate: false, trustedConnection: false, } } function getDBPool() { const poolPromise = new sql.ConnectionPool(SQL_CONFIG) .connect() .then(pool =&gt; { console.log('Connected to MSSQL'); return pool }) return poolPromise; }if (require.main === module) { main();}You can find the full code at Github as well.Hope the above helps someone. Please drop a comment if you have any questions." }, { "title": "Converting Java CLI Client to a Native Executable with GraalVM", "url": "/posts/ConvertingJavaClientToANativeExcutableWithGraalVM/", "categories": "Programming", "tags": "GraalVM, wso2", "date": "2022-10-16 00:00:00 -0400", "snippet": "GraalVM is a high-performance JDK distribution designed to accelerate the execution of applications written in Java and other JVM languages along with support for JavaScript, Ruby, Python, and a number of other popular languages. GrallVM allows you to compile your Java code into native executables which allows you to run them without a JRE. So in this post, I’ll explain the steps I followed to get my CLI client to work with GraalVM.Some background, my CLI client was kind of a legacy application that wrapped a couple of SOAP services. Hence its dependencies required components like Axis2, Axiom, log4j etc. Initially, I used args4j for interactive CLI inputs and the Maven shade plugin to build an executable uber Jar.There are a couple of ways to set up and use GraalVMs native-image. You can use the GraalVM Docker images, or you can simply install GraalVM on your machine. If you are super lazy you can also use Github Actions workflow with GraalVM setup step. But I highly recommend using the Docker image or setting it up locally which will save a lot of time when debugging issues. For this, you can refer the official Installation GuideThe first try, I thought it was super easy. So I had my Uber Jar created and I simply executed the below, thinking everything would work.native-image -jar capp-manager.jarAt least some success, with no errors when converting to a native image.Top 10 packages in code area: Top 10 object types in image heap: 663.86KB java.util 947.79KB byte[] for code metadata 351.87KB java.lang 897.25KB java.lang.String 273.76KB java.text 837.11KB byte[] for general heap data 234.86KB java.util.regex 628.55KB java.lang.Class 198.40KB com.oracle.svm.jni 544.92KB byte[] for java.lang.String 193.86KB java.util.concurrent 434.86KB java.util.HashMap$Node 146.93KB java.math 224.30KB com.oracle.svm.core.hub.DynamicHubCompanion 120.62KB java.lang.invoke 209.58KB java.util.HashMap$Node[] 105.90KB com.oracle.svm.core.genscavenge 164.34KB java.lang.String[] 98.16KB java.util.logging 155.81KB java.util.concurrent.ConcurrentHashMap$Node 1.98MB for 119 more packages 1.54MB for 786 more object types------------------------------------------------------------------------------------------------------------------------ 0.3s (2.0% of total time) in 17 GCs | Peak RSS: 3.30GB | CPU load: 8.23------------------------------------------------------------------------------------------------------------------------Produced artifacts: /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/target/capp-manager-0.1.3 (executable) /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/target/capp-manager-0.1.3.build_artifacts.txt (txt)========================================================================================================================Finished generating 'capp-manager-0.1.3' in 14.0s.Warning: Image 'capp-manager-0.1.3' is a fallback image that requires a JDK for execution (use --no-fallback to suppress fallback image generation and to print more detailed information why a fallback image was necessary).Although there were no errors, there was a fishy warning at the end, but let’s move the executable to a different location and run it. Bummer, the first error.Error: Could not find or load main class org.wso2.capp.client.MainCaused by: java.lang.ClassNotFoundException: org.wso2.capp.client.MainThe reason is the last Warning, for some weird reason the default behavior of native-image command is to create a fallback image, which means if the executable fails it will execute the Jar as a fallback measure. So this Jar needs a JVM to run. This doesn’t make any sense as to why they have this behavior by default, given the whole purpose of converting to a native image is to run the native image without a JVM. But I found there are legal reasons for Oracle to have this built this way. If interested you can read more from hereSo let’s add the --no-fallback flag and create the image, which will stop creating the fallback image and pack everything into the native image.native-image -jar capp-manager.jar --no-fallbackThe output------------------------------------------------------------------------------------------------------------------------Top 10 packages in code area: Top 10 object types in image heap: 706.01KB java.util 1.06MB byte[] for code metadata 370.62KB java.lang 980.19KB java.lang.String 274.10KB java.text 887.39KB byte[] for general heap data 234.89KB java.util.regex 773.05KB java.lang.Class 204.67KB java.util.concurrent 648.69KB byte[] for java.lang.String 202.26KB com.oracle.svm.jni 435.38KB java.util.HashMap$Node 146.93KB java.math 282.73KB com.oracle.svm.core.hub.DynamicHubCompanion 119.09KB java.lang.invoke 210.28KB java.util.HashMap$Node[] 105.90KB com.oracle.svm.core.genscavenge 176.99KB java.lang.String[] 98.30KB java.util.logging 155.81KB java.util.concurrent.ConcurrentHashMap$Node 2.50MB for 152 more packages 1.60MB for 829 more object types------------------------------------------------------------------------------------------------------------------------ 0.4s (2.3% of total time) in 18 GCs | Peak RSS: 3.47GB | CPU load: 7.80------------------------------------------------------------------------------------------------------------------------Produced artifacts: /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/target/capp-manager-0.1.3 (executable) /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/target/capp-manager-0.1.3.build_artifacts.txt (txt)========================================================================================================================No Warning this time, so let’s execute it. Once executed I got another error.Exception in thread \"main\" java.lang.ExceptionInInitializerError\tat org.apache.logging.log4j.LogManager.&lt;clinit&gt;(LogManager.java:61)\tat org.wso2.capp.client.Main.&lt;clinit&gt;(Main.java:14)Caused by: java.lang.IllegalStateException: java.lang.InstantiationException: org.apache.logging.log4j.message.DefaultFlowMessageFactory\tat org.apache.logging.log4j.spi.AbstractLogger.createDefaultFlowMessageFactory(AbstractLogger.java:246)\tat org.apache.logging.log4j.spi.AbstractLogger.&lt;init&gt;(AbstractLogger.java:144)\tat org.apache.logging.log4j.status.StatusLogger.&lt;init&gt;(StatusLogger.java:105)\tat org.apache.logging.log4j.status.StatusLogger.&lt;clinit&gt;(StatusLogger.java:85)\t... 2 moreCaused by: java.lang.InstantiationException: org.apache.logging.log4j.message.DefaultFlowMessageFactory\tat java.lang.Class.newInstance(DynamicHub.java:639)\tat org.apache.logging.log4j.spi.AbstractLogger.createDefaultFlowMessageFactory(AbstractLogger.java:244)\t... 5 moreCaused by: java.lang.NoSuchMethodException: org.apache.logging.log4j.message.DefaultFlowMessageFactory.&lt;init&gt;()\tat java.lang.Class.getConstructor0(DynamicHub.java:3585)\tat java.lang.Class.newInstance(DynamicHub.java:626)\t... 6 moreIf we look at the above error, it’s complaining about some methods not being found. Inorder to understand the reason for the above type of errors we need to understand how native images are created. When creating the native image, the native-image tool performs static code analysis of the code to determine the classes and methods that should be reachable in the application runtime. At the same time, GraalVM employs different strategies to improve performance. For example, given Class initialization degrades the performance in the runtime, GraalVM does something called Class Initialization In Build time You can read more about these on the official GrallVM website.Looking at the above log4j error specifically, GrallVM will have trouble with file descriptors to be referenced in static fields because the files might not be present at run time. Similar issues can occur if Java Reflection is used to load classes in the runtime, native-image will not be able to predict which classes will be needed in the runtime. At this point, you can try to force native-image tool to pack the classes that are needed when building the image.Let’s see how to do this. For this, you can use the Tracing Agent that comes with GraalVM which allows you to trace the classes being loaded in the application runtime. This agent will allow you to run your application and generate a configuration file that will tell native-image tool which resources/classes need to be included. The Tracing Agent is capable of detecting usages of the Java Native Interface (JNI), Java Reflection, Dynamic Proxy objects (java.lang.reflect.Proxy), or class path resources (Class.getResource).Let’s run the client with the agent using -agentlib:native-image-agent flag. If you have a server-type application that keeps on running. Once run with the agent you can execute all the options in your application which will load the classes in the runtime and the agent will track them. In my case I have a CLI client which runs and exits, so I had to run the CLI client multiple times with different options that are supported and generate the configurations. One thing to note is if you run the command multiple times with the agent the configurations will be overridden, hence like in my case if you have multiple commands you may want to use multiple output directories for each command and merge the config files later. You can use something like meld to check the diff between files and merge them.# Option 1java -agentlib:native-image-agent=config-output-dir=./gral-settings -jar capp-manager-0.1.3.jar list-apps --server https://localhost:9443 --username admin --password admin# Option 2java -agentlib:native-image-agent=config-output-dir=./gral-settings2 -jar capp-manager-0.1.3.jar download --server https://localhost:9443 --username admin --password admin --app-name HelloCompositeExporter --destination ./...After merging the files you would have multiple configuration files like below. If you see anything missing you can add them manually to each config file.graalvm-settings├── jni-config.json├── predefined-classes-config.json├── proxy-config.json├── reflect-config.json├── resource-config.json└── serialization-config.jsonLet’s build the new native image with the above config files. Since my application does http/https calls I’m passing some additional flags like --enable-url-protocols=https,http --enable-https --enable-http if your application doesn’t need them, you can omit them.native-image -jar target/capp-manager-0.1.3.jar --no-fallback -H:ConfigurationFileDirectories=./graalvm-settings --enable-url-protocols=https,http --enable-https --enable-httpThe output Top 10 packages in code area: Top 10 object types in image heap: 1.49MB sun.security.ssl 5.82MB byte[] for code metadata 1.04MB java.util 2.71MB java.lang.String 839.46KB picocli 2.66MB java.lang.Class 742.50KB com.sun.org.apache.xalan.internal.xsltc.compiler 2.57MB byte[] for general heap data 704.42KB com.sun.crypto.provider 2.04MB byte[] for java.lang.String 534.36KB java.lang 966.80KB com.oracle.svm.core.hub.DynamicHubCompanion 530.98KB java.lang.invoke 900.18KB byte[] for embedded resources 500.51KB com.sun.org.apache.xerces.internal.impl 639.19KB java.util.HashMap$Node 487.96KB c.s.org.apache.xerces.internal.impl.xs.traversers 561.73KB byte[] for reflection metadata 459.58KB sun.security.x509 559.87KB java.lang.String[] 19.49MB for 483 more packages 4.69MB for 2238 more object types------------------------------------------------------------------------------------------------------------------------ 2.0s (4.1% of total time) in 31 GCs | Peak RSS: 6.35GB | CPU load: 8.25------------------------------------------------------------------------------------------------------------------------Produced artifacts: /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/capp-manager-0.1.3 (executable) /home/yasassri/workspace/projects/capp-manager/wso2-capp-manager/capp-manager-0.1.3.build_artifacts.txt (txt)========================================================================================================================Finished generating 'capp-manager-0.1.3' in 47.7s.You can see the number of packages that are added to the native image is much higher than in the previous build. The more classes you add to the native image the size of the executable will increase. So you may consider optimizing it after you have successfully built the native image. After the above steps, my application started working without any further errors.General Tips Try to use dependencies that are GraalVM compatible. In my case, I migrated from Args4J to Pocoli to build the interactive CLI wchih is compatible with GraalVM.Hope the above helps. Please drop a comment if you have any questions." }, { "title": "Stop A Recurring Jenkins Job After a Fixed number of Runs", "url": "/posts/StopJenkinsRecurringJobAfterFixedNumberOfExecutions/", "categories": "Automation, CICD", "tags": "jenkins, cicd, devops, groovy, automation", "date": "2022-10-06 00:00:00 -0400", "snippet": "This solution was inspired by a Stackoverflow question I happen to answer. The requirement is not very common but thought of sharing it if anyone ever needs it. The requirement is something like this, once a Jenkins Job is triggered manually, it should run X number of builds, running one build every day and then stop after X number of runs. There is no such functionality in Jenkins to support this kind of behavior OOB, so how can we achieve this? On top of my head, I thought maybe we can play with the Cron expression configured in a Job dynamically to run and then stop. So what I ended up doing is adding a Cron expression to run every day if manually triggered and then removing the corn expression after a predefined number of runs elapse. The next problem is how would we determine how many runs it ran. Jenkins doesn’t have an OOB way to persist and pass Build data between different builds, so the easiest way to get around this is to check each build before the current build and determine whether it has executed X number of Build by a Timer Trigger. When a build is triggered in Jenkins, it’s associated with a cause. In other words, how the Build was triggered. For example, you may have, “User Triggers”, “Timer Triggers”, “Remote Triggers” etc. so you should be able to retrieve this information and use them in your flow.Let’s come up with a Groovy function we can use within the Pipeline to return the correct Cron expression based on the number of runs and the type of trigger.def getCron() { def runEveryDayCron = \"0 9 * * *\" //Runs everyday at 9 def numberOfRunsToCheck = 7 // Will run 7 times def currentBuildNumber = currentBuild.getNumber() def job = Jenkins.getInstance().getItemByFullName(env.JOB_NAME) for(int i=currentBuildNumber; i &gt; currentBuildNumber - numberOfRunsToCheck; i--) { def build = job.getBuildByNumber(i) if(build.getCause(hudson.model.Cause$UserIdCause) != null) { //This is a manually triggered Build return runEveryDayCron } } return \"\"}The above function will traverse through the past builds and determine whether it has elapsed the expected number of Builds, if so the Cron expression will be set to blank preventing it from running again. Once you execute the Build again(Manually) the Cron expression will be set to run every day. The logic is pretty simple.Next, let’s set the Con expression to the Pipeline. Check the following complete Pipeline with the Groovy function embedded. Once executed the correct Cron will be retrieved and set to the Pipeline.def expression = getCron()pipeline { agent any triggers{ cron(expression) } stages { stage('Example') { steps { script { echo \"Build\" } } } }}def getCron() { def runEveryDayCron = \"0 9 * * *\" //Runs everyday at 9 def numberOfRunsToCheck = 7 // Will run 7 times def currentBuildNumber = currentBuild.getNumber() def job = Jenkins.getInstance().getItemByFullName(env.JOB_NAME) for(int i=currentBuildNumber; i &gt; currentBuildNumber - numberOfRunsToCheck; i--) { def build = job.getBuildByNumber(i) if(build.getCause(hudson.model.Cause$UserIdCause) != null) { //This is a manually triggered Build return runEveryDayCron } } return \"\"}Hope the above helps. Drop a comment if you have any questions." }, { "title": "Custom Github Action for WSO2 APICTL", "url": "/posts/CustomGithubActionForWSO2APICTL/", "categories": "Automation", "tags": "Github, cicd, wso2, automation", "date": "2022-10-04 00:00:00 -0400", "snippet": "I have worked with Github Actions in some projects and used quite a few Actions written by others. So I wanted to write my own custom action to experience the process. So here I’m with this post after building a custom GitHUb Action to setup WSO2 APICTL. This post will not explain how to build a custom action but rather how to use the custom action that was built. The custom Action is located in the Guthub Market place at https://github.com/marketplace/actions/setup-wso2-apictlWhat is WSO2 API CTLWSO2 API Controller (apictl) is a command-line tool providing the capability to move APIs, API Products, and Applications across environments and to perform CI/CD operations. Furthermore, it can perform WSO2 Micro Integrator (WSO2 MI) server specific operations such as monitoring Synapse artifacts and performing MI management/administrative tasks from the command line.Why the Custom ActionIf you are working on Github and using Github workflows to manage your deployments and to develop CICD pipelines, for any CICD pipeline related WSO2 APIM or MI this action can be used. This custom GH action will basically setup WSO2 APICTL in a Linux environment which can be used to perform different operations like, moving APIs between environments etc. Since there are no usecases for setting up on Windows environments, currently only Linux environments are supported.How to use the ActionBasically in your workflow you can use the action by adding the following step. (Make sure you get the latest version from Github)uses: yasassri/setup-wso2-apictl@v1.2with: version: '4.1.0'You are setup to any version of the APICTL by passing the version parameter. Or If you wish to install APICTL by providing a URL to a tarball location, that will also work. Following are the parameters you can pass as inputs.version[Optional] The version of the APICTL to setup. The default vale is \"4.1.0\"The version will be picked from the Github releases at https://github.com/wso2/product-apim-tooling/releases. Ex: 4.1.0, v4.1.0, 3.2.5 etc.tarball_location[Optional] A location to an APICTL Tarball if it needs to be downloaded from a custom location. No DefaultExamples of different inputsuses: yasassri/setup-wso2-apictl@v1.2with: version: '4.1.0'uses: yasassri/setup-wso2-apictl@v1.2with: tarball_location: 'https://github.com/wso2/product-apim-tooling/releases/download/v4.1.0/apictl-4.1.0-linux-x64.tar.gz'Full workflowFollowing is a full workflow with the custom action.name: WSO2_APIMon: [push]jobs: wso2: runs-on: ubuntu-latest permissions: issues: write pull-requests: write steps: - uses: yasassri/setup-wso2-apictl@v1.2 with: version: 'v3.2.5' - run: apictl version" }, { "title": "Create Jenkins Credentials Through Rest API", "url": "/posts/CreateJenkinsCredentialsThroughRestAPI/", "categories": "CICD", "tags": "jenkins, cicd, devops, groovy, automation", "date": "2022-07-10 00:00:00 -0400", "snippet": "In this post I’ll explain how you can create Jenkins credentials through the Jenkins Rest API. Let’s get into it straight away.How To Authenticate Jenkins APIInorder to Authenticate the API call you have to pass an API token and the Jenkins Crump with the API call. Jenkins Crunb was introduced to prevent CSRF attacks. Let’s see how exactly you can get all these details.$JENKINS_URL : This variable refers to Jenkins URL with the custom context if you have any.$JENKINS_USER : Username of the user used to generate the access token$JENKINS_PASSWORD : Password of the user used to generate the access token$API_ACCESS_TOKEN : The access token.$JENKINS_CRUMB ; Jenkis Crumb. First we need to get the Jenkins crumb by passing the Basic Auth header. We also need to save the Cookies so we can use the same Cookies when doing the API request for this I’m using --cookie-jar option with curl.curl -s --cookie-jar /tmp/cookies -u $JENKINS_USER:$JENKINS_PASSWORD $JENKINS_URL/crumbIssuer/api/jsonThe above will give you a response like the below.{ \"_class\": \"hudson.security.csrf.DefaultCrumbIssuer\", \"crumb\": \"e6aa50dfdda70b3db256d27a1effe7e0be5033b94d9edeaa9e108c212e91f4c2\", \"crumbRequestField\": \"Jenkins-Crumb\"}From the above, you can extract the crumb value and pass it with the header Jenkins-Crumb to generate a token. Send the following curl request to Generate an Access Token.curl -u \"$JENKINS_USER:$JENKINS_USER_PASS\" -H $JENKINS_CRUMB -s \\ --cookie /tmp/cookies $JENKINS_URL'/me/descriptorByName/jenkins.security.ApiTokenProperty/generateNewToken' \\ --data 'newTokenName=GlobalToken'You will get the following response for the above call. Extract the value for tokenValue and use it as the Access Token on consecutive API calls.{ \"status\": \"ok\", \"data\": { \"tokenName\": \"GlobalToken\", \"tokenUuid\": \"cef3f33d-5e61-4d5e-a966-44d52546f5aa\", \"tokenValue\": \"1135b180fcc6ba2cbc0d3fb04621d8700a\" }}SummaryFollowing are all of the above commands together. You can execute all of the following commands and generate an Access token.Note: Following commands need curl and jq. Execute in the same session.# Change the following appropriatelyJENKINS_URL=\"http://localhost:8080\"JENKINS_USER=adminJENKINS_USER_PASS=admin# Get the Crumb**JENKINS_CRUMB=$(curl -u \"$JENKINS_USER:$JENKINS_USER_PASS\" -s --cookie-jar /tmp/cookies $JENKINS_URL'/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)')#Get the Access token**ACCESS_TOKEN=$(curl -u \"$JENKINS_USER:$JENKINS_USER_PASS\" -H $JENKINS_CRUMB -s \\ --cookie /tmp/cookies $JENKINS_URL'/me/descriptorByName/jenkins.security.ApiTokenProperty/generateNewToken' \\ --data 'newTokenName=GlobalToken' | jq -r '.data.tokenValue')Creating credentialsYou need to generate the approprite payload to create Credentials depending on the type of the credentials. You can refer the following for this.Payload for Create Username Password Credential&lt;com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl&gt; &lt;id&gt;TestCredentials&lt;/id&gt; &lt;description&gt;This is sample&lt;/description&gt; &lt;username&gt;admin2&lt;/username&gt; &lt;password&gt;admin2&lt;/password&gt;&lt;/com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl&gt;Payload for SSH User Private Key&lt;com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey&gt; &lt;id&gt;SSHCredential&lt;/id&gt; &lt;description&gt;&lt;/description&gt; &lt;username&gt;ubuntu&lt;/username&gt; &lt;usernameSecret&gt;false&lt;/usernameSecret&gt; &lt;privateKeySource class=\"com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey$DirectEntryPrivateKeySource\"&gt; &lt;privateKey&gt;PRIVATEKEY_HERE&lt;/privateKey&gt; &lt;/privateKeySource&gt;&lt;/com.cloudbees.jenkins.plugins.sshcredentials.impl.BasicSSHUserPrivateKey&gt;Payload for Github App Credentials&lt;org.jenkinsci.plugins.github__branch__source.GitHubAppCredentials&gt; &lt;id&gt;GIthubApp_YCR&lt;/id&gt; &lt;description&gt;&lt;/description&gt; &lt;appID&gt;GitAppID&lt;/appID&gt; &lt;privateKey&gt;PRIVATE_KEY&lt;/privateKey&gt; &lt;apiUri&gt;&lt;/apiUri&gt; &lt;owner&gt;OWNER&lt;/owner&gt;&lt;/org.jenkinsci.plugins.github__branch__source.GitHubAppCredentials&gt;Payload for Secret Text Content Credential&lt;org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl&gt; &lt;id&gt;SecretTextYcr&lt;/id&gt; &lt;description&gt;&lt;/description&gt; &lt;secret&gt;SECRET_TEXT&lt;/secret&gt;&lt;/org.jenkinsci.plugins.plaincredentials.impl.StringCredentialsImpl&gt;Payload for X509 Cert/Docker Server Credential&lt;org.jenkinsci.plugins.docker.commons.credentials.DockerServerCredentials&gt; &lt;id&gt;X509Cert&lt;/id&gt; &lt;description&gt;&lt;/description&gt; &lt;clientKey&gt;CLIENT_KEY&lt;/clientKey&gt; &lt;clientCertificate&gt;CLIENT_CERTIFICATE&lt;/clientCertificate&gt; &lt;serverCaCertificate&gt;SERVER_CERT&lt;/serverCaCertificate&gt;&lt;/org.jenkinsci.plugins.docker.commons.credentials.DockerServerCredentials&gt;Add the content of the payload to a file named credentials.xmlConstructing the API URL for creating credentialsThe credential create URL format is JENKINS_URL/credentials/store/CREDENTIALS_STORE_NAME/domain/DOMAIN_NAME/ You need to change this appropriately based on the location and the domain you are creating the credentials under. The easiest way to get this URL is by navigating to an existing credential from the UI and copying the URL.Once you figure out the correct URL(context path) for the API call execute the following command.curl -u $JENKINS_USER:$ACCESS_TOKEN \\ -H $JENKINS_CRUMB \\ -H 'content-type:application/xml' \\ \"$JENKINS_URL/credentials/store/system/domain/_/createCredentials\" \\ -d @credentials.xmlHope the above helps!! Happy Coding." }, { "title": "Role Based Authorization Hanlder for WSO2 Micro Integrator", "url": "/posts/AuthorizationHandlerForWSO2MI/", "categories": "WSO2, WSO2MI", "tags": "wso2, wso2mi, java, authorization", "date": "2022-06-05 00:00:00 -0400", "snippet": "WSO2 Micro Integrator(MI) is a lightweight integration platform that allows you to write different integrations to connect systems just like an Enterprise Service Bus. If you are familiar with the WSO2 products stack, WSO2 MI is the descendent of Enterprise Integrator and MI will be eventually replacing EI.When creating different services that serve your enterprise traffic it’s important to keep your services secure. For example, once you create an API or a Proxy service in WSO2 MI you should be able to enforce Authentication and Authorization. Authentication will determine whether the user trying to access the service is valid or whether the user is registered in the system and Authorization will determine whether the registered user has privileges to access a service. In other terms, authentication validates your credentials and authorization will check whether you have permissions assigned to you. Normally the permissions are determined y the roles that are assigned to a specific user.With the default product, there is no way to enforce authorization for an API/Proxy , hence I came up with a custom Authorization handler. The next section will explain how you can use the authorization handler.Custom Authorization HandlerThe handler is capable of handling authentication and authorization separately. The user can disable authorization and only use authentication if necessary. Following is a simple flow diagram that illustrates the logical flow.As you can see above if the handler is engaged in a service, it will do authentication first and if authorization is enabled the authorization will be checked. When adding authorization the users can define a list of roles that are allowed to access the service.How to use the Authorization Handler First build this project or download the released Jar from https://github.com/yasassri/wso2mi-authorization-handler/releases/tag/v1.0.0 and copy the wso2-authorization-handler-*.jar to &lt;MI_HOME&gt;/lib directory.Note: Make sure you download the latest release. Then in your API/Proxy service definition you can add the handler as shown below.&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"test2\" context=\"/test2\" binds-to=\"default\"&gt; &lt;resource methods=\"POST\" binds-to=\"default\"&gt; &lt;inSequence&gt; &lt;payloadFactory media-type=\"xml\"&gt; &lt;format&gt; &lt;response&gt;Hello&lt;/response&gt; &lt;/format&gt; &lt;args/&gt; &lt;/payloadFactory&gt; &lt;respond/&gt; &lt;/inSequence&gt; &lt;outSequence/&gt; &lt;faultSequence/&gt; &lt;/resource&gt; &lt;handlers&gt; &lt;handler class=\"com.ycr.auth.handlers.AuthorizationHandler\"&gt; &lt;property name=\"roles\" value=\"admin,test\" /&gt; &lt;property name=\"authorize\" value=\"true\" /&gt; &lt;/handler&gt;&lt;/handlers&gt;&lt;/api&gt; Then add the user credentials as a Basic Auth header to your request and send the request. Refer following. (Make sure user:password is base64 encoded)curl -v -X POST http://localhost:8290/test2 -H \"Authorization: Basic YWRtaW46YWRtaW4=\"Note: If the Authentication fails you will get a HTTP 401 or if the Authorization fails you will receive a HTTP 403.Handler params.Handler accepts two parameters roles and authorize.&lt;handler class=\"com.ycr.auth.handlers.AuthorizationHandler\"&gt; &lt;property name=\"roles\" value=\"admin,test\" /&gt; &lt;property name=\"authorize\" value=\"true\" /&gt;&lt;/handler&gt; roles: The user can define a list of allowed roles for the API. authorize: If Authorization(Role validation) is not required this can be set to false. If set to false only authentication will take place. The authorization stage will be skipped.Note: In order to do role management you need to plugin an LDAP or a JDBC user store to MI.Full source code can be ound hereHope this post helps. Happy Coding!" }, { "title": "How to use Windows Authentication for MSSQL with WSO2 Servers.", "url": "/posts/How-to-use-Windows-Authentication-forMSSQLwithWSO2Servers/", "categories": "WSO2, Generic", "tags": "wso2, windows, mysql", "date": "2021-05-30 01:33:03 -0400", "snippet": "Every WSO2 product require a DB to persist different data. If we take WSO2 API Manager there are multiple DBs used to store different sets of data, for example, APIM DB, Registry DB, User DB etc. By default these databases are pointed to a H2 Database which resides in the file system. For production deployments these databases should be externalised by pointing it to a production grade Databases like MSSQL, MYSQL, Oracle etc.In this post I will discuss how WSO2 servers can be connected to MSSQL using Windows Authentication. For this post I will take API Manager 3.2.0 to demo the configurations.When it comes to different authentication methods for MSSQL there are mainly following two methods. SQL Server Authentication Windows AuthenticationWhen it comes to Windows Authentication there are again two methods of authentication. Kerberos based authentication and local authentication. In this post I will be explaining about local authentication. Inorder for this to work your WSO2 server needs to run on a Windows server. In this method the Server will simply use the local windows authentication to connect to the MSSQL Server.So let’s get Started!!First make sure you have enabled Windows authentication in your MSSQL Server. After this you are ready to configure the WSO2 server. Follow the steps below inorder to confiure Windows authentication.Step 01Download the MSSQL driver. You can download the relevant driver from https://www.microsoft.com/en-us/downloadIn my case I will be using the following driver version. https://download.microsoft.com/download/4/c/3/4c31fbc1-62cc-4a0b-932a-b38ca31cd410/sqljdbc_9.2.1.0_enu.zipStep 02Then unzip this driver archive. The content will look like something similar to below. From here on I will refer this directory as Step 03Now copy the mssql driver jar from the unziped directory to APIM_HOME/repository/components/lib directory. Make sure you select the correct driver which matches the Java runtime version you have in the VM.Step 04Next in the navigate to /_auth//_ where you will see a dll file. This dll will be used by the driver to initiate the authentication. Copy the path to this .dll. ()Step 05Now we need to configure the path so that JDBC driver can find the above dll file. Inorder to do that open /bin/wso2server.bat and add the following line as shown below.After adding the following line the batch file will look something similar to below.Step 05Now in the /repository/conf/deployment.toml change the datasource configurations as shown below. Note the additional connection parameter **integratedSecurity=true.** Also add some dummy value as the username and keep the password empty.Step 06Now start the WSO2 server by running the wso2server.bat file. If everything is in place the server should start without any errors.That’s about it, drop a comment if you have any questions." }, { "title": "Building Your WSO2 Integrations with Unit Tests; A Guide for WSO2 Micro Integrator & Enterprise…", "url": "/posts/Building-Your-WSO2-Integrations-with-Unit-Tests-A-Guide-for-WSO2-Micro-IntegratorEnterprise/", "categories": "WSO2, Micro Integrator", "tags": "wso2, wso2mi, testing", "date": "2021-03-31 01:02:13 -0400", "snippet": "WSO2 Micro Integrator(MI) and Enterprise Integrator(EI) is a feature rich Opensource Integration platform which provides traditional Enterprise Service Bus capabilities combined with modern cutting edge features. Developing and deploying integration use cases with WSO2 MI/EI is similar to developing any kind of application, which follows a similar lifecycle. When developing an integration use case, you have the source code, which consist of unit tests and when the source is compiled, the unit tests are executed and it produces a deployable artefact. In this post I will unveil few strategies you can adopt when running unit tests for your integrations.As mentioned WSO2 Micro Integrator and Enterprise Integrator provides the capability to the users to write unit tests to make sure the integration are working as expected. In order to develop integrations the users can use Integration Studio which is an Eclipse based IDE capable of providing users with Drag and drop capabilities as well as source editing capabilities. Also it allows you to write Unit tests and to run these tests easily using the Integrations Studio. The major focus of this post is to discuss how can you execute these tests using maven when the artefacts are built in a CICD pipeline. I’m assuming you already know how to write unit tests in WSO2 Integrations Studio and know basic build commands with Maven.Let’s get started :)How to Run Unit tests?The unit tests run when we are performing a maven build on the source, WSO2 has developed a maven-unittest-plugin inorder to run tests which is trigggered by a maven build. Running your unit tests is not straight forward when building using a CICD pipeline. This is because inorder to execute the unit tests maven needs a WSO2 MI/EI server present and accessible. When running the tests they will be deployed to the server and tested. In a nutshell there are two different ways to run unit tests. Using a local MI Server runtimeIn this way the MI runtime should resides in the same machine we are building the source code. When executing the maven build command we need specify the path the mvn clean install -DtestServerType=local -DtestServerPath=/mnt/servers/wso2mi-1.2.0/bin/micro-integrator.sh Using a remote MI Server runtime.In this way we can use a already running MI server runtime and we can specify the host/IP and the port of the remote server. mvn clean install -DtestServerType=remote -DtestServerHost=10.5.2.13 -DtestServerPort=9008Now let’s look at how we can implementing a CICD Pipeline in a realworl scenario.Method 01; Using a Remote MI ServerIn this method, we are using a remote MI server to execute Unit tests, If you have a MI up and running already you can use this as well. But I will be setting up a MI runtime from the build pipeline itself. The easiest way to setup a MI runtime is to use a MI docker container. Hence I will be using docker to start a MI server and to implement the CICD pipeline I will be using Jenkins.So let’s look at the implementation. The pipeline will first clone the integration source code from Github, then it will start an MI Container and then build the source with unit tests. The units tests will run on the MI container we are starting. Additionally we can deploy the artefacts to a lower environment and run integration tests on top of the integrations as well. The pipeline will simply look something similar to below.The Jenkins pipeline code that is used given below. Please note that following code is for demo purposes and may not be production ready.The full pipeline and the sample project can be found here.Method 02; Build your source within a ContainerIn this method we will be using a local MI server to run unitests. But we will not be setting up anything in the Jenkins agent nodes, instead we will setup a container with all the build dependencies and build the source within this container. Here we need a custom docker image with a few dependencies. Basically we need to setup maven in the Docker image so we can build the source within the Docker container. We will be using WSO2 MI base image to create the image. You can find the docker resources here.Inorder to build the custom image follow the steps below. Clone the repo https://github.com/yasassri/unittest-build-demo Navigate to resources/docker and execute the following command.docker build -t  .I have already built the image and pushed it into the public docker registry. Hence you can use the image ycrnet/wso2mi-1.2.0-unittest:1.0.0 image as well.After building the image and pushing it to a registry you can use the following pipeline to build the source with unit tests.The pipeline will look like something below.Following is the source code of the Jenkins pipeline and the full pipeline source is available here.That’s about it. Please drop a comment if you have any questions." }, { "title": "Keeping Your WSO2 Products up-to-date in a Containerised Environment", "url": "/posts/Keeping-Your-WSO2-Products-up-to-date-in-a-ContainerisedEnv/", "categories": "WSO2, Generic", "tags": "wso2, docker, wso2am, wso2ei, wso2mi, wum", "date": "2021-03-01 12:59:10 -0500", "snippet": "Modern-day enterprises and businesses rely on a variety of integral applications/systems to fulfill their business requirements. These systems can vary from a simple HR system to a complex API Management platform. Given that businesses are highly dependent on these different underline systems that are integrated into their core business requirements, it’s crucial to keep these systems up to date.WSO2 is one such Opensource software provider which has a variety of products to cater to different business requirements. You can read more about WSO2 from here. In this post, I’ll discuss how WSO2 Products can be updated within a containerized environment. In other words how WSO2 updates can be pulled to a containerized environment.What this post will not cover is how to update a complete platform(including configurations etc.) with WSO2 Updates. This post will simply cover what’s the best strategy to pull WSO2 updated base images to your environment. Also, this post will not cover the standard Update process in a non-container environment. If you want to learn about this you can refer this.Also, although I will be referring WSO2 MI in most examples this post is applicable to all WSO2 Products.WSO2 Releases and Updates.In general WSO2 follows a release cycle and this can be quarterly, annually etc. depending on the product release strategy. After a product release is done, if “product bugs” or “security vulnerabilities” or “product improvements” are found, WSO2 products are patched and updated. Even after a newer version of a Product is released, WSO2 updates will be provided up until that particular product version reaches EOL. One thing to note is, WSO2 updates are available for customers with an active subscription. (For free users you can always build the product from the source if a bug is being fixed in the source :))Why Update WSO2 Products?In a nutshell “To Avoid System Failures and Vulnerabilities”. Continuous maintenance of your software solution ensures system health and security throughout its lifetime. In a nutshell, the following are the benefits of frequently updating your WSO2 product. Utilizing all available updates eliminates the possibility of being stymied by a known issue during your development. A customer request often results in an improvement, or a fix that is built, well-tested, and delivered to you as an update. WSO2 Updates give you immediate access to a surge of improvements, packaged for easy deployment into your production systems, making sure the deployment is solid and secure. Update Services are available for WSO2 releases for Ten years, thus you can exploit bug and security fixes while remaining free to manage your upgrade schedule. WSO2 carefully monitor hundreds of open source projects, collect and assess security reports from users or academia, run code security reviews, and automate code analysis to identify and address possible security weaknesses.How would Updates Affect your Integrations.When it comes to updates the updates can be provided to different components of the product. If you are familiar with WSO2 Micro Integrator, updates can be introduced to specific mediators, the transports layers etc. So depending on which component the updates are introduced this will directly impact your integration use cases. Hence it’s important to understand what kind of updates went in and what are the changes the users will have to do in-order to make sure the your integrations are working as expected.WSO2 Update Versioning.Inorder to come up with a proper update strategy we need to understand how WSO2 updates are versioned. When it comes to maintaining updates It’s crucial to properly version the images inorder to identify the correct update levels and the changes that are incorporated in that particular update.Initially when the product does a GA release, the base version of the image will be versioned as 1.2.0.0.(This is WSO2 MI version) Then consecutive updates will be bumping the last digit of the versioning string. Versioning will follow a standard like the below.1.2.0.0 [GA] — -&gt; 1.2.0.1 [Update] — -&gt; 1.2.0.2 [Update]As shown above, the last digit of the version number increments as updates are released. This number can be used to determine the update level and the changes that went into the specific update level. Since we are using docker images to pull the updates, the docker image will carry a slightly different versioning format as shown below. The versioning will carry the base OS type as well. The following example shows the alpine based image.1.2.0.0-alpine [Latest]&lt; — -&gt; 1.2.0.1-alpine [Update] — –&gt; 1.2.0.2-alpine [Update]The tags in the WSO2 docker registry will look like something below, Official Docker Repositories — Tags (wso2.com)As shown above, the 1.2.0.0-alpine will be the docker image tag with the latest changes. Initially, image 1.2.0.0-alpine will have the GA pack which will be continuously updated/replaced as updates are released. The Docker image metadata will consist of the actual update level of the image.Inorer to check the metadata of the image you can use the docker inspect command. Once you have inspected the latest image the update level details will be shown as below.So when we are pulling the base image from the WSO2 private repository we can read the metadata of the docker image and determine the actual update level of the latest tag. When pushing the base image to your private registry we can maintain the same update level as the tag to easily identify the update level. This is depicted in the below diagram.Maintaining the actual WSO2 update level internally will help the users to learn which changes have gone into the available docker image easily.Checking the Update DetailsInorder to check the update details you can visit WSO2 updates Portal. https://updates-info.wso2.com/Following is the standard update view and WSO2 is currently working on a generic view that can be used for docker updated images. I’ll update the post once this is rolled out to production.Update details of a specific update will be shown as below,Implementing a Sample PipelineThe Pipeline can simply use a DockerFile to pull the WSO2 Base image, pack necessary dependencies and push the image to the internal registry. Following is a reference Docker file that can be used to build the image.FROM docker.wso2.com/wso2mi:1.2.0.0-alpine LABEL image=\"Private Base Image\" # Copy resources if needed, Keystores etc. COPY resources/custom.jks /home/wso2carbon/wso2mi-1.2.0/ # Add Remote resources if needed, common drivers etc. ADD http://source.file/url /destination/pathInorder to extract Docker metadata and to build the base image following script can be used. (Following was extracted from a Bamboo pipeline hence note the environment variables that are being used.)docker pull ${bamboo.wso2DockerRegistry}:${bamboo.wso2DockerImage}; updateLevel=$(docker inspect -f \"\" ${bamboo.wso2DockerRegistry}:${bamboo.wso2DockerImage} | tr -d \\\\\") if \\[ -z \"$updateLevel\" \\]; then echo \"ERROR: The Update level was not found in the Image. Aboarting the process!!\" exit 1 else echo \"The Update level is set as $updateLevel\" fi cd resources/docker docker build -t ${bamboo.privateRegistry}/${bamboo.privateWSO2ImageName}:$updateLevel.${bamboo.buildNumber} . docker login ${bamboo.privateRegistry} -u ${bamboo.privateRegistryUserName} -p ${bamboo.privateRegistryPassword} docker push ${bamboo.privateRegistry}/${bamboo.privateWSO2ImageName}:$updateLevel.${bamboo.buildNumber} echo \"Image ${bamboo.privateRegistry}/${bamboo.privateWSO2ImageName}:$updateLevel.${bamboo.buildNumber} pushed Successfully!\"The full Bamboo pipeline for the above can be found at yasassri/wso2-baseimage-update-bamboo: Bamboo Spec repository to pull WSO2 updated Base images. (github.com)References: https://updates.docs.wso2.com/en/latest/" }, { "title": "Push Your Container logs without a Side Car; Example with WSO2 MI, Newrelic and Fluent-Bit", "url": "/posts/Push-Your-Container-logs-without-a-Side-Car/", "categories": "Devops, Observability", "tags": "observability, newrelic, wso2mi, k8s, fluent", "date": "2021-01-11 23:31:20 -0500", "snippet": "Kubernetes has become the defacto standard for Container orchestration when deploying large-scale applications. It provides an easy abstraction for efficiently managing large-scale containerized applications with declarative configurations, an easy deployment mechanism, observability, security, and both scaling and failover capabilities. As with any type of application, collecting and analyzing logs is crucial when observing the applications, to make sure the applications are running smoothly. Given K8S is a platform that can run hundreds of Pods at a given time, having a centralized Logging mechanism enhances the developer experience which allows them to easily monitor the logs.The common standard way to push your container logs into a centralized logging mechanism is by using a Sidecar container within your pod. But in case, if you want to avoid using a sidecar and if you need to push the logs to a third-party log analyser you can get an idea of how to do it by reading this post. In this post, I will explore how you can leverage daemon sets to push specific container logs to a log analysing system. In order to set things up, I will be using Newrelic and Fluent-Bit to get this done. And as the application, I will be using WSO2 MI, but you can run any application.Why Not a Side-Car.As I mentioned earlier the side-car is the standard way to push logs from your application container. But the side car can consume resources since it will spin-up a dedicated container to read the log files per application container. So the resource consumption can be high since the number of logging containers will be equal to number of application containers. Hence instead of a side-car we can use a Daemonset which will only create a Daemon process per K8S minion node. The main problem with a common Daemonset for all the pods is how one can differentiate, filter and group logs as required. We will talk about this as well in this post.How Kubernetes Logging WorksThe standard way for an application to log, is to push the application logs in to the standard out(std-out). What ever that is written to the std-out will be shown by the kubectl logs command. All the logs that are generated from the containers(all the std-out streams generated from all the containers) will be written to files in the minion(worker) node where the pod is running. By default these logs get written in to “/var/log” directory.The pod related logs will be stored in /var/log/pods/ and the content of this directory will look like following.As shown in the above image the pods directory have sub directory to represent each pod. The structure within the log directory doesn’t have a folder hierarchy to represent different namespaces, deployments etc. But simply follows a naming convention to differentiate the logs. Also in-order to make the automation process of reading these logs easy, the container log files are sym-linked to files in /var/log/containers. The contents in /var/log/containers are depicted in the below image.As shown above, the log files in the /var/log/containers directory are sym-linked with the actual log files available in the /var/log/pods directory. If we look closely at the log file we can see a naming convention again. Understanding this naming convention is important when filtering the relevant log files which needs to be pushed to the log analyzer. The file naming format is similar to below.**__-\\*.log**Filtering the LogsIn order to filter out the logs we can use naming conventions and regex patterns. Fluent-bit allows you to pick log files based on a regex pattern. As mentioned earlier the log file pattern is something similar to below, hence this pattern can be used when filtering the logs.__-\\*.logIf you want to select all the logs in a specific namespace you can use a regex pattern similar to below.__\\*.logIf you want to select specific pods in a specific namespace, first you have to come up with a naming convention for your containers. For example you can add a known prefix/postfix for the name of your container, so if we name out application container as “wso2mi-integration\" wso2mi is the prefix we will be using to filter the logs. The regex to filterout containers in a specific namespace with the prefix wso2mi will look something similar to below.\\*__wso2mi-\\*.log### Building the Solution#### ComponentsIn order to demonstrate the solution I will be using two technology stacks. As the application Stack I will be using [WSO2 Micro Integrator](https://ei.docs.wso2.com/en/7.2.0/micro-integrator/overview/introduction/) and as the log analysing platform I will be using [Newrelic](https://newrelic.com/) which is a cloud hosted platform.#### Use-caseBefore building the use-case let’s try to understand what are we going to build. In this use case we will have multiple WSO2 Micro Integrator Pods running in different namespaces. We will be pushing the MI logs in each name space to a different account(Space) in Newrelic.#### ArchitectureThe solution at a very highlevel can be captured as shown in the below image.![](/assets/img/medium/0__5Wd__RBHt5a__TcTTg.jpg)As shown above for each filter criteria(e.g: Per name space, Per container group etc.) we will be running a daemon set. These Daemonsets are responsible to reading the relevant logs and pushing them to the correct Newrelic account.#### Setting up the Components**Deploying the application**In order to deploy the application to K8S environment I will be using helm charts provided by WSO2.1. First I will create a new K8S namespace to deploy the application.```shkubectl create namespace test-tenant```2. Then clone the helm chart repository and checkout the correct tag.git clone [https://github.com/wso2/kubernetes-mi.git](https://github.com/wso2/kubernetes-mi.git) ```shcd kubernetes-mi git checkout tags/v1.2.0.1```3. Then navigate to helm/micro-integrator directory and issue helm install command.```shhelm install wso2-mi . -n test-tenant```4. Now you can check whether MI is deployed by browsing the resources.```sh&gt; kubectl get all -n test-tenant```![](/assets/img/medium/1__S7yV6QB7poN__Fr__nejXA7A.png)#### **Installing the Daemonset**Newrelic has a Fluent-bit based plugin to read and push logs. Lets install the plugina and start pushing logs.#### Installing the Fluent-Bit output adapterWe will be installing the Newrelic’s Fluent Bit output plugin as a daemonset as illustrated above. For each tenant space we will be creating a new daemonset. Follow the instructions below install the daemonset.**Step 01*** Clone the following helm repository.```shgit clone [https://github.com/newrelic/helm-charts](https://github.com/newrelic/helm-charts).git```**Step 02*** In the cloned repository, navigate to helm-charts/charts/newrelic-logging/ and open the values.yaml file* In the values.yaml file add the license key of the Newrelic account as shown below. (You can generate a license key from your newrelic account)![](/assets/img/medium/0__hsjGPgCWUZ3hkOYP.jpg)* In the same file specify the relevant regex pattern to filter-out the correct log files. Note the naming convention of the log files when creating the pattern. __-\\*.log A sample log pattern is \\*__\\*.log![](/assets/img/medium/0__Xtmmi7tfBq46Hk1W.jpg)**Step 03*** Now install the helm chart. Note that I will be installing the Daemonset to the default namespace.```shhelm install tenant01-logger . ```* You can check the created resources as shown below. The ready count should match with the number minion nodes you have in your K8S cluster.![](/assets/img/medium/0__NPgrVyBYxDOS2VL4.jpg)#### Checking the logs in NewrelicInorder to check the application logs you needs to login to the Newrelic Dashboard and logs will be available in the logs section.![](/assets/img/medium/0__yUHn9LIdUGpmTMlM.jpg)In Newrelic the logs can be filtered based on different attributes etc. To check what are the log related attributes you can click on a log entry.![](/assets/img/medium/0__agN3xsA7XiLW1s__P.jpg)Happy Coding!!!" }, { "title": "Empowering your Teams; Building an Integration Mashup as a Service with WSO2 MI", "url": "/posts/EmpoweringTeamsBuildinganntegrationMashupasaServicewithWSO2MI/", "categories": "", "tags": "java, aws", "date": "2020-12-21 04:42:10 -0500", "snippet": "In medium-scale to large-scale corporations, there are numerous teams/business-units that operate cohesively yet independently to drive the organization towards its’ goals and objectives. These teams typically use different integral systems within their business units, which adds up to many different systems been adopted within the Organization. As the number of different systems grows within an Organization it’s going to be a nightmare to manage these different systems and technologies. On the other hand, it’s going to incur a considerable cost to the Organization. Hence as a proactive measure, it’s ideal for an organization to streamline and standardize the technology stack and the process, where the organization can build a common platform that can be used by the business units while eliminating the maintenance and management overhead. In simple terms, the Organization can develop a centralized SaaS platform where different business units can onboard themselves and start developing their integrations with a few clicks without worrying about the service management aspect. We will call this an “Integration Mashup as a Service(IMaaS)” where the platform will facilitate self-onboarding, service reuse, scalability, resilience, and above all streamline the integrations across the entire organization.In this post, I will be exploring how WSO2 MI can be leveraged to develop an Integration Mashup as Service, Platform.TerminologyTenant: A dedicated service space within the IMaaS platform that is separated from other business units where the relevant teams can build their services.Integrations: Integrations include connecting systems, exposing your data as services, protocol switching to connect with different systems, exposing APIs, syncing different systems, payload transformations, processing files and updating systems, etc.Service: An collection of integrations that will cater a business requirement.Designing the platformKey PrinciplesBefore diving into the design of the platform it’s crucial to identify the key principles that need to be considered when designing a SaaS platform. Hence the IMaaS platform should be developed keeping the following key principles in mind. Scalability. Performance. Implementability. High Availability. Observability. Data Privacy/Isolation. Updatability.Let’s look at each of the above principles in detail.ScalabilityScalability is the ability of a system to handle a growing amount of work by adding resources to the system. Hence, the services should be able to scale depending on the load. This allows us to build a stable and fault-tolerant platform.PerformancePerformance expectancy is a key factor when designing any service platform. Hence the platform should be able to operate at an acceptable performance threshold.ImplementabilityThis refers to how easily the integrations can be built as microservices within the platform. This is a key factor in attracting new users to the platform.High AvailabilityThis captures the platform’s ability to maintain zero downtime. Which covers the aspects like failover management and the ability to perform rolling updates without shutting down the services.ObservabilityOne of the critical capabilities of any platform should be the ability to observe the services. Observability covers a few different aspects like, server resource monitoring, server matrics, message tracing, and log aggregating. Observability plays a key role when troubleshooting issues in the systems.Data Privacy/Isolation.This captures the platforms’ ability to isolate different tenant spaces and the ability to maintain data privacy. This requirement may differ from one organization to another.Updatability.This captures the platform’s ability to update the services seamlessly and the ability to update the platform it-self easily.High-level DesignThe IMaaS will have multiple tenant spaces and each tenant space can run multiple services within the same space. This can be depicted as shown in the following diagram.Honoring the key principles that were discussed in the previous section the platform can be designed to support the following capabilities. Hence a single tenant space will look something like below.As shown in the above diagram, the services would run in a dedicated space and they can run as a single service or as clusters. At the same time, they can interact with other services. Overall the platform should support Log aggregation, message training, service matrics, service scaling, failover/rolling updates, and CICD.Taking a step back and giving the platform a high-level look, at a very high-level the platform will look like something similar to the below diagram.As shown in the above image each tenant space should be isolated and each tenant space will have a dedicated control plane to mage the services. Also, an observability stack will be attached which allows developers to monitor the services and the platform. A state controller will make sure the services are rolled-out as per the developers’ requirements and the states are maintained properly. The state controller will also make sure the services are scaled appropriately.Selecting the correct technology StackNow we have an idea of the key principles we need to follow when designing and developing the platform. Let’s see how we can select the ideal technologies to cater the core principles.Integration PlatformThe integration platform is the key component of the IMaaS platform which will let the tenant users build their services. So selecting and a proven, easy to adopt and container friendly tool is crucial in many ways. If the learning curve is too steep it will not be popular among the users. As the integration Platform, we will be using WSO2s’ MicroIntegrator which is an open-source, cloud-native integration framework with a graphical drag-and-drop integration flow designer and a configuration-based runtime for integrating APIs, services, data, and SaaS, proprietary, and legacy systems. Which makes it an ideal candidate for the IMaaS.InfrastructureSelecting an appropriate infrastructure is also crucial to make your platform reliable, resilient and manageable. One important thing to note is how much out of box features are available to full fill the platform requirements. For example, if rolling updates, failover, security, etc. are provided OOB from the infra layer it will take less effort to build up the platform. Hence given the micro nature of the services and segregation of the service spaces, the obvious option is Kubernetes. So as the underline infrastructure we could use an environment like K8S or any other variant of K8S like Openshift, EKS, etc.Workflow and Source ManagementIn order to manage tenant onboarding, service onboarding, and service lifecycle management we will use a Gitops based approach, hence we will be using Github. We will also use Github to manage tenant-specific source code.CI and CDAs the main CD tool, we can use Kubernetes native CD tools like ArgoCD or Spinnaker. I’m in favor of ArgoCD as we are following a Gitops based service lifecycle management process. Also as the CI tool, we can use a tool like Jenkins.ObservabilityWSO2 MI supports different observability standards/tools out of the box which gives it an edge over other vendors available in the market. So as the observability stack we can use Grafana, Prometheus, Jaeger, Loki, and Fluent-Bit. Which allows monitoring the Server matrics, message tracing, and server log aggregation.As a summary following are the tools we will be using for the solution we are building. K8S: The underline infrastructure. WSO2 MicroIntegrator: The integration platform. Jenkins: CICD tool which takes care of the source building and overall deployment process. Spinnaker/ArgoCD: CD tool used to roll-out/roll-back changes to K8S. Helm: Configuration automation tool used for K8S artifacts. Github: Gitops based operation management and source control. Grafana: Centralized dashboard for tracing, metrics, and logs. Prometheus: Used to capturing Matrics data. Loki and Fluent-bit: Used to index the application logs.Reference ArchitectureComponent ArchitectureNow let’s look at a reference design we can create with the aforementioned toolset. At a glance, each tenant space would be something similar to the below component diagram. The following image shows where each component falls within the platform.In the above diagram, we have the WSO2 Micro-Integration stack, Observability stack, and other components that are needed to make the platform cater to the user requirements. We will talk about each flow in detail in the following sections.Tenant and Service Onboarding and Lifecycle ManagementWe will be following a Git-ops based approach to onboard and manage tenant services. Hence the entry point for tenant onboarding will be a Git repo itself. Also, service onboarding and service lifecycle management will be done via Gitops. In order to adopt the Git-ops strategy, a repository structure can be designed as shown below.As depicted in the above image, the platform will have a central repo where tenants can self onboard into the platform by creating a tenant metadata file. Then the tenants can onboard their services by adding service-related metadata to the tenants’ metadata file. When a tenant-specific metadata file is committed to this repo, a separate namespace will be created in K8S for this particular tenant. Each Tenant repository will be pointing to multiple service repositories where it will have three branches representing each environment(Dev, Test, and Prod). The tenants should be able to omit the creation of environments if they wish to do so. Deployment of services to different environments will be handled by the commits happening in each of these branches. For example, the developers can keep on working on the dev/test branch, and when they are ready to release to the production they can simply merge the test branch with the production which will trigger a release pipeline.Now let’s look at how the tenant onboarding happens with the Gitops approach. The following image depicts the tenant onboarding flow.As shown in the above diagram initially a tenant user will send a PR to the IMaaS Repository which needs to be approved by platform admins. Once approved the changes will be picked up by a Jenkins Job and necessary resources for the tenant will be created. The process will create a dedicated namespace in K8S, a dedicated Docker registry in a private docker registry, and required CD pipelines in ArgoCD. Once the tenant is onboarded the tenants can start onboarding the services.Service Lifecycle ManagementThis section explains how the service lifecycle can be managed. The following image depicts the CICD process of a tenant service. This process captures the flow that happens when the developers are moving the source from their local environments up to the development/test environment.The above flow has two independent flows. The CI flow and the CD flow. The CI flow starts when the developers push the source code to the integration source repo or if they manually trigger the pipeline. First, the pipeline builds the integration source and runs unit tests on the source, after the source is built the deployment artifacts will be packed to a docker image and then the image is pushed to a Docker registry. After the updated docker image is pushed the CD process will be triggered by adding a commit to the tenant service repo. This will be picked by the CD job, the helm charts will be updated which will be picked by ArgoCD, and then the Environment will be updated with the latest changes.Once the developers are confident that the development work is completed and the integrations are stable to be released to the production they can follow the below flow which captures production release process.In the above flow in order to trigger the release process, the developers have to merge the staging/dev branch with the production branch. Merging into the production will trigger the production release.That’s what I will be covering in this post. I’m planning to write a few follow-up articles to cover the implementation details of the proposed solution.Feel free to drop a comment if you have any queries!!" }, { "title": "Setting up Nexus On Docker", "url": "/posts/Setting-up-Nexus-On-Docker/", "categories": "Devops, Docker", "tags": "docker, nexus", "date": "2020-06-03 02:27:56 -0400", "snippet": "In this post, I will be explaining how to set up a nexus repository with docker.Nexus is an artifact repository which manages software “artifacts” required for the development. If you develop software, your builds can download dependencies from Nexus and can publish artifacts to Nexus creating a way to share artifacts within an organization.Prerequisites: Docker should be installed in your machine and the machine should have an active internet connection.So let’s get started.Step 01First, we need to create a directory in the host machine to persist nexus data. If you do not do this data will be lost upon container restart.Create a directory and set correct permissions using the following command.mkdir /some/dir/nexus-data sudo chown -R 200 /some/dir/nexus-dataStep 02Then start the nexus docker image with the following command.sudo docker run -d -p 8081:8081 — name nexus -v /home/ubuntu/nexus/nexus-data:/nexus-data sonatype/nexus3:3.23.0Then check whether the container is runningsudo docker psNow copy the container ID from the output of the above command.Then let’s check the logs of the container.sudo docker logs -f &lt;CONTAINER ID&gt; or sudo docker logs -f nexusIf you see the above logs that means the nexus server started successfully.Step 03Now in order to access the admin console you need to get the admin password. Which will be generated in the nexus-data directory the password will be stored in a file called. /nexus-data/admin.password.After grabbing the password from this file navigate to the following URL.[**http://:8081/**](http://192.168.104.81:8081/)And click on login and enter admin as the username and add the password you just grabbed earlier.Then you will be asked to complete a Wizard which will let you change the password and set a few additional configurations. And finally you should be able to see your repositories.Now we have a running Nexus repository set with docker. Please drop a message if you have any questions." }, { "title": "Manage WSO2 Carbon Applications Remotely.", "url": "/posts/Manage-WSO2-Carbon-Applications-Remotely/", "categories": "WSO2, WSO2 EI", "tags": "java, wso2, wso2ei, automation, cli, cicd", "date": "2020-04-28 00:12:19 -0400", "snippet": "In this post I’ll introduce you to a Java CLI client that I happened to write to manage WSO2 Carbon Applications. WSO2 Enterprise Integrator (was known as WSO2 ESB) has a mechanism to develop integration flows externally using Integration Studio and deploy them using .car files. This allows you to change environment specific variables and then deploy the same carbon application (Capp) between multiple environments.Why do we need a Client.When we are dealing with Capps we have very limited options to manage the Capps effectively in remote servers. Although WSO2 provides a maven capp deployer plugin which is not really useful when we are dealing with proper CICD pipelines. This particular client is useful when we are writing CICD pipelines to manage integration development and deployment. This client will allow you to take backups, check the apps deployed and then properly version and deploy Capps. I will write a separate post covering how we can write a proper CICD pipeline for WSO2 EI development.WSO2 Carbon Application Manager.This is a very simple client which utilizes WSO2 Admin Services to manage Carbon Applications that are deployed. The very high level architecture is something like below,Building the Client.The source for this client resides at https://github.com/yasassri/wso2-capp-manager. First go to the repo https://github.com/yasassri/wso2-capp-manager and clone it. Then go to the project root where the POM resides and execute the following command. mvn clean install This will create an executable uber Jar in the target directory. Refer the following screenshot.Importing certificates to access remote servers.In-order to access the remote servers we need to have remote server certificates in a client truststore so the SSL connections can be created with proper server name validations. Hence first we need to create a java keystore in JKS format and then import the remote servers public certificate to that keystore. You can refer to the following command to import the certificate into the keystore. e.g: keytool -import -alias dev-env -file public.cer -storetype JKS -keystore client-truststore.jksAfter creating the keystore we can refer to this keystore when executing the client.Executing the client.The Capp Manager supports the following operations at the time I’m writing this post. Deploy Apps Undeploy Apps List Apps Download AppsFollowing section will explain how the above features can be used.Common Parameters for the Client.The client basically requires following parameters to execute. Following parameters are common to all the sub commands.–server: used to specify the server URL E.g: –server https://localhost:9443–trustore-location: Specify the location of the client trustore. E.g: –trustore-location ./client-truststore.jks–trustore-password: Specify the password of the trustore E.g: –trustore-password wso2carbon–username: The server access username. E.g: –username admin–password: Password for the server access user. E.g: –password adminDeploy CAppsThe deploy operation allows you to deploy a given carbon application.java -jar capp-manager-1.0.0.jar deploy --server [_https://localhost:9443_](https://localhost:9443) --trustore-location ./client-truststore.jks --trustore-password wso2carbon --username admin --password admin --file ./cicd-demo-capp_1.0.1-SNAPSHOT.carOutputIf the app you are trying to deploy already exist it will throw the following error. A app already exists with the name cicd-demo-capp_1.0.1-SNAPSHOT.carAlso you can use the –force option which will undeploy an existing carbon app and deploy the new application.java -jar capp-manager-1.0.0.jar deploy --server [https://localhost:9443](https://localhost:9443) --trustore-location ./client-truststore.jks --trustore-password wso2carbon --username admin --password admin --file ./cicd-demo-capp_1.0.1-SNAPSHOT.car --forceUndeploy CAppThis operation allows you to undeploy a specified CApp. You have to specify the carbon application name only.java -jar capp-manager-1.0.0.jar undeploy --server [_https://localhost:9443_](https://localhost:9443) --trustore-location ./client-truststore.jks --trustore-password wso2carbon --username admin --password admin --app-name cicd-demo-cappOutputList AppsThe list operation allows you to list all the carbon applications that are already deployed in the server.java -jar capp-manager-1.0.0.jar list-apps --server [_https://localhost:9443_](https://localhost:9443) --trustore-location ./security/client-truststore.jks --trustore-password wso2carbon --username admin --password admiOutputIf you want to get a processable output you can only read the standard out. In this case you can direct the standard error to a different stream.java -jar capp-manager-1.0.0.jar list-apps --server [_https://localhost:9443_](https://localhost:9443) --trustore-location ./client-truststore.jks --trustore-password wso2carbon --username admin --password admin 2&gt; /dev/nullOutputDownload CAppThe download operation allows you to download the specified carbon application to a given location.java -jar capp-manager-1.0.0.jar download --server [_https://localhost:9443_](https://localhost:9443) --trustore-location ./client-truststore.jks --trustore-password wso2carbon --username admin --password admin --app-name cicd-demo-capp --destination ./OutputUnderstanding the output streamsAll the output logs are written into a file and to the console STD_ERROR. All the output data is written to the console std out. We are writing outputs to the std error because the outputs that should be processable will be written to the stdout. So the output information can be easily processed. The log file will be created at /logs/capp-client.logsIf you only wants the standard error printed in the console you can pipe the std error to a different file.e.g: java -jar capp-manager-0.1.2.jar help 2&gt; /dev/nullFuture Work Read server configurations from a config file. Allow the user to specify the Capp Version. Allow redeploying apps through CLI." }, { "title": "Exposing Openshift Route with a Loadbalancer.", "url": "/posts/Exposing-Openshift-Route-with-a-Loadbalancer/", "categories": "Devops, Openshift", "tags": "loadbalancing, openshift, nginx, haproxy, tcp", "date": "2020-03-16 12:53:06 -0400", "snippet": "Openshift is the commercial Kubernetes offering that is built and backed by RedHat. Although the underline runtime of Openshift is Kubernetes there are few differences when it comes to Openshift from Kubernetes. Mainly traffic routing, scaling/rollbacks, and deployments are different in Openshift. I’m not going to talk much about Openshift or Kubernetes in this post. In this post, I will explain how we can front an Openshift Route with an external load balancer.Route is the mechanism that allows you to expose an Openshift service externally, this is similar to an Ingress in Kubernetes. By default, a route in Openshift is an HA Proxy. When you create a route it will expose your services through an HA Proxy router. There is a Controler for the router which dynamically updates the router configurations as it detects changes. But the pluggable architecture of Openshift routes allows you to change this to any other implementation you like and it supports plugins like F5. You can read more information about this from the Openshift website itself.Before we start the configurations we need to understand how the actual routing happens. When a request is received to the Openshift router the router will use the SNI(Server Name Identifier) information in the HTTP handshake to identify the service the request should be routed to. For example, if there are multiple services that are running in Openshift, the service calls are differentiated with the SNI information. You can refer to the following as an example.In this post, we look at how we can front the Openshift route with an external load balancer. There are many occasions you will have to do this. In most cases, within an organization, the internal applications will be exposed to the internet through a Loadbalancer to isolate the networks. In my case, I had two different Openshift clusters (DR and Production) which I had to loadbalance between, So I had to front these two Openshift clusters with an F5 load balancer. In this post, I will not use F5, but I will be using Nginx as an application Loadbalancer and HAProxy as a TCP loadbalancer. Following is how the end-result will look.From the above diagram, it seems like a simple thing to do but it isn’t. As you can see I have two hostnames to access the services, one hostname for the internal users and one for the external users. Since Openshift routes rely on SNI information in the handshake to decide the service the request should be sent it’s not straight forward to achieve this.There are multiple ways to achieve this, I will explain each method in the below section. Using a TCP LoadbalancerThe easiest way to achieve this is using a TCP loablancer to stream the requests directly to the route, but what’s important is in this case you need to create two different routes, one for the public DNS and the other with the internal Openshift DNS. Traffic thats originating externally will reach the route with the public DNS and the others to the internal route.I have used HAProxy to demonstrate this. Following is the haproxy.cfg I used to achieve this.global log 127.0.0.1 local0 log 127.0.0.1 local1 debug maxconn 4096 defaults log global mode http option httplog option dontlognull retries 3 option redispatch maxconn 2000 timeout connect 5000 timeout client 50000 timeout server 50000 frontend is.wso2.com bind *:443 mode tcp default_backend nodes backend nodes mode tcp balance roundrobin server server1 192.168.64.7:443 check2. Using an Application Loadbalancer.This is the more complex approach where you will terminate the connection at the main LB and initiate a new connection with the Openshift Route. As I mentioned earlier Openshift route identifies the appropriate route with the SNI information. So I had to set the SNI information when the second connection is made from the External Loadbalancer to Openshift Route.Following is the NginX configuration that you can use to achieve this. Please note the important properties like proxy_ssl_name, proxy_ssl_server_name which are used to set new SNI information for the connection.server { listen 443 ssl; server_name employee.external.com; ssl_certificate /usr/local/etc/nginx/keys/wso2.crt; ssl_certificate_key /usr/local/etc/nginx/keys/wso2.key; location / { proxy_set_header Host employee.apps-crc.testing; proxy_pass https://192.168.64.7; proxy_redirect https://employee.apps-crc.testing/ https://employee.external.com/; proxy_ssl_name employee.apps-crc.testing; proxy_ssl_server_name on; }}In the above solution, you can use the single route for exposing the services externally and internally. But I would recommend using solution 1 for the above problem to make things simple.Please drop a comment if you have any questions." }, { "title": "Restricting Access to WSO2 Servers", "url": "/posts/Restricting-Access-to-WSO2-Servers/", "categories": "WSO2, Generic", "tags": "wso2, wso2am, wso2ei, security", "date": "2019-06-29 02:23:28 -0400", "snippet": "In this post I’ll explain how we can restrict access to WSO2 Carbon console for specific client machines. In WSO2 servers they have an embedded tomcat instance to host the web-apps including the server. So here we will be using tomcat valves to restrict access to the server. I’ll be covering few different scenarios where you will have to use different configurations and valves.White List a IP When Accessing the Server DirectlyTake a scenario where the WSO2 server is accessed directly without a load balancer. So in this case how can we whitelist only requests generating from a particular IP or block requests originating from a particular IP.In-order to do this we need to use tomcats’ RemoteAddrValve. Follow the steps below to add the valve. Open /repository/conf/tomcat/carbon/META-INF/context.xml and add the valve configurations as below,&lt;Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"10.100.5.112\"/&gt; Then restart the server for changes to be effective.Above filter will only allow requests that are originating from the 10.100.5.112 IP. Also note the value for the allow property accepts a regex, so you can add a proper regex pattern here. There are few attributes you can set in the valve. You can also add multiple IPs, deny access to specific IPs etc. You can refer the tomcat documents https://tomcat.apache.org/tomcat-7.0-doc/config/valve.html#Remote_Address_Filter for more details on this.White List a Host Name When Accessing the ServerIn this case we are validating whether the requests are generated for a specific Host. By default Tomcat doesn’t do a DNS lookup on the clients IP to determine the Host name if the client. So we need to enable this in the relevent tomcat connector. After enabling dns lookup you have to use RemoteHostValve in tomcat to validate the host. Follow instructions below to do this. Open To do that open “/repository/conf/tomcat/catalina-server.xml\" and enable _enableLookups_ property. Full connector configurations will look like following.&lt;Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"9443\" bindOnInit=\"false\" sslProtocol=\"TLS\" sslEnabledProtocols=\"TLSv1,TLSv1.1,TLSv1.2\" maxHttpHeaderSize=\"8192\" acceptorThreadCount=\"2\" maxThreads=\"250\" minSpareThreads=\"50\" disableUploadTimeout=\"false\" **enableLookups=\"true\"** connectionUploadTimeout=\"120000\" maxKeepAliveRequests=\"200\" acceptCount=\"200\" server=\"WSO2 Carbon Server\" clientAuth=\"want\" compression=\"on\" scheme=\"https\" secure=\"true\" SSLEnabled=\"true\" compressionMinSize=\"2048\" noCompressionUserAgents=\"gozilla, traviata\" compressableMimeType=\"text/html,text/javascript,application/x-javascript,application/javascript,application/xml,text/css,application/xslt+xml,text/xsl,image/gif,image/jpg,image/jpeg\" keystoreFile=\"${carbon.home}/repository/resources/security/wso2carbon.jks\" keystorePass=\"wso2carbon\" URIEncoding=\"UTF-8\"/&gt; Next open /repository/conf/tomcat/carbon/META-INF/context.xml and add the following valve configuration,&lt;Valve className=\"org.apache.catalina.valves.RemoteHostValve\" allow=\"wso2.mydomain.com\"/&gt; Then restart the server for changes to get effective.Now if you try to access the WSO2 server with https://wso2.mydomain.com:9443/carbon you will be able to access but if you try to access the server with a different host you will be access denied. You can get more details on this valve from here https://tomcat.apache.org/tomcat-7.0-doc/config/valve.html#Remote_Host_FilterWhite List a Client IP When Accessing the Server through a LBWhen you are accessing WSO2 servers through a LB the connections will be made as follows.The client will create a connection with the LB(Load balancer) and the LB will be creating a connection with the backend server. So for the LB the client will be 10.100.5.112 and for the WSO2 server the client will be the LB. So WSO2 server will always see the clients IP as 192.168.112.8 irrespective from which client machine the request is generating from. So in-order to get the clients IP we need to read the X-Forwarded-For header, when ever a request is parsing through a proxy the clients IP will be appended to the X-Forwarded-For header. So the first IP in the X-Forwarded-For header is the IP of the request originating client.Inorder to get the original clients IP we need to use tomcats RemoteIpValve this will read the X-Forwarded-For header and set the original clients IP as the request generating clients IP. After we get the remote clients IP we can use a Access Control valve to white list the requests. So in this case we need to use two different valves together.Please follow instruction below, Open &lt;WSO2_HOME&gt;/repository/conf/tomcat/carbon/META-INF/context.xml and add the following valve configurations,&lt;Valve className=\"org.apache.catalina.valves.RemoteIpValve\"/&gt; &lt;Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"10.100.5.112\"/&gt; Then restart the server for changes to get effective.Now if you try to access the WSO2 server with the client 10.100.5.112 you will be allows to access WSO2 servers. But for other clients the requests will be blocked. You can use what ever valve you desire in cinjuction with the RemoteIpValve." }, { "title": "WSO2 APIM Manager on Openshift", "url": "/posts/WSO2-APIM-Manager-on-Openshift/", "categories": "WSO2, API Manager", "tags": "wso2, devops, openshift, k8s", "date": "2019-06-05 00:27:05 -0400", "snippet": "In this post I will be explaining how WSO2 products can be deployed on top of Openshift. By the time I was writing this blog I was using Openshift 3.11 and this deployment is hardcoded for MSSQL.Openshift is the commercial Kubernetes offering that is provided by RedHat. Although the underline runtime is Kubernetes there are few differences when it comes to Openshift from Kubernetes. Mainly traffic routing, scaling/rollbacks and deployments are different in Openshift. Feature wise the main deferences are, Routes Deployment Configs Image streams etc.Routes are similar to ingress controllers in Kubernetes and routes are responsible for exposing the services externally. Deployment configs are similar to Deployments in Kubernetes but the internally it operated differently. It is true that we can use most of the kubernetesconfigurations within openshift as it is, but using Kubernetes artifacts as they are won’t let you utilize the added features that are available in Openshift.In order to utilize the features that are provided by openshift you should use openshift specific object types instead of Kuberenetes objects.For example you can use Kubernetes deplyments instead of Openshifts Deployment configs but using Deployment configs will allowyou to use some neat features that are provided by Openshift, like adding triggers to deployments, undoing deployments, monitor deployment process and add triggers to your deployment to perform rolling updates etc. Some of these capabilities are not available with Kubernetes Deployments.In this deployment I will be focussing on doing a production ready deployment, where you will do proper backing up of logs etc.The related artifacts are located in https://github.com/yasassri/openshift-wso2-apim From here on I will refer this repository as Understanding the repository structure.There are several directories in the . Mainly docker, k8s, scripts, and resources. You can see the repository structure in the below image.In the following section what each directory contains will be explained.Docker artifactsDocker directory contains the docker artifacts to create docker images that are required by the openshift environment. If we look closely at the docker folder, it has the following content.Each product/profile has its own docker resources to create the necessary docker images. So if we consider a single directory which represents a single product it has the following folder structure.Files directory contains the necessary external artifacts that the Docker build process requires, for example, any external dependencies (SQL drivers) that the wso2 servers require should be added to the lib directory. The init.sh script is the entry point of the created docker container. So if you want to perform a task on docker container startup you should add this to the init.sh script. More details on docker image creation will be in a different section of this document.K8S ArtifactsThen we have the k8s directory. Which contains all the artifacts that are required to setup WSO2 servers on top of openshift. If we look closely it has the following folder structure.This structure is different from the docker structure since the grouping is done based on deployment patterns and a deployment contains multiple products and should be started in a specific order to make sure the deployment starts without errors and it is successful, so the products/profiles are grouped in a logical manner. If we take the apim-is directory it contains, WSO2 API Manager, Identity Server, API Manager Analytics worker and IS Analytics worker related artifacts. The folder structure would look like following,Each directory named as the product will have K8S artifacts for that product. Typically it will have a K8S deployment yaml, a service yaml and in some cases a volume claim. As an example refer the following,Then for the configurations maps, we use the confs directory which will contain the configuration files that are needed for the config maps. For each product it will have a directory in the configs folder.The above configurations will be mounted to the container on startup to alter the default configurations of WSO2 products. Only the configuration files that needs alterations are included here. These will be mounted as secrets or config maps depending on the requirement. The configurations doesn’t have all the values hardcoded, most configuration files will have place holders in them and these will be populated when deploying. For example if we take the carbon.xml the hostname of the server will be populated when deploying. So the configuration file will have a placeholder like below.More information on these placeholders and how they get replaced will be provided in the later section of the document.Then in the folder structure we have a routes directory which will contain the openshift route configurations. Following are the routes of the apim-is deployment.Then within the K8S folder we have a directory called rbac and volumes.Rbac directory has the access control configurations that are required by the cluster user to get clustering information. The volumes has the configurations to create persistent volumes.Resources for the DeploymentThe resources directory holds the Db scripts and other relevant resources that are needed for the deployment.Then we have the scripts directory which contains all the automation scripts to prepare the deployment artifacts and then to deploy WSO2 servers. How to use these scripts will be explained later.Deploying WSO2 serversCreating necessary Databases and Tables for WSO2 serversWSO2 servers require DBs to persist the application and user data, so we need credentials to access a database. This particular DB user should have and Read/Right access to table and Table create permissions with the created Databases.In the /resources/db-setup directory it has the following scripts. First inorder to create the necessary databases execute the 1-databases.sql script on the MSSQL Server. This will create the necessary DBs. Then execute all the scripts within the subfolders to create the necessary tables in the DBs. For example apim/mssql.sql, common_um/mssql.sql. There is no order for the execution, we need to execute all the scripts. Executing the above scripts will create the necessary databases along with the tables. Now create a user who has aforementioned permissions to access the above databases.Creating the key-stores for the serversYou can follow the steps in the below section to create a new keystore with a private key and a new public certificate. We will be using the keytool that is available with your JDK installation for creating the keys-stores. Note that the public key certificate we generate for the keystore is self-signed. Therefore, if you need a public key that is CA-signed, you need to generate a CA-signed certificate and import it to the keystore.Change the CN name appropriately depending on the environment and execute the following command in the terminal. You can specify any value for the keytool -genkey -alias wso2carbon -keyalg RSA -keysize 2048 -keystore wso2carbon.jks -dname “CN=\\*.apps.wso2.com, OU=BP,O=WSO2,L=EC,S=WS,C=EC\" -storepass &lt;KEY_PASS&gt; -keypass &lt;KEY_PASS&gt; -validity 7300Here we are using *.apps.wso2.com as the CN for the self signed certificate. You need to change this depending on your environment. is the only value you should be changing. Also note that the storepass and the keypass has to be the same.Next fom the created keystore above lets extract the public certificate from the keystore. To do that execute the following command. This will store the public key in a file called wso2pub.pem.keytool -export -alias wso2carbon -keystore wso2carbon.jks -file wso2pub.pemNow let’s import this to the client-trustore.jks of WSO2 server. From any WSO2 pack copy the client-trustore.jks and from where the keystore is located execute the following command to delete the existing public certificate. When prompted for the password enter wso2carbon as the password.keytool -delete -alias wso2carbon -keystore client-truststore.jksThen execute the following command to import the new public certificate we created.keytool -import -alias wso2carbon -file wso2pub.pem -keystore client-truststore.jks -storepass &lt;KEY_PASS&gt;Next execute the following command to change the default password of the client-trstore. When prompted for the existing password enter wso2carbon as the password.keytool -keystore client-truststore.jks -storepass wso2carbon -storepasswdYou can refer the following documentation for more information on using an existing private key and a certificate to create the necessary keystores. https://docs.wso2.com/display/ADMIN44x/Creating+New+Keystores#CreatingNewKeystores-CreatinganewkeystoreDownloading WSO2 Product distributions and updatingFor the deployment we need WSO2 distributions, there are few ways to download WSO2 distributions. We will use WSO2 Update Manager to download distributions in this case.Inorder to download wso2 product distributions or any updates you need to setup WSO2 Update manager(WUM) tool. You can refer the following document to setup the WUM tool (https://wso2.com/wum/download). So you should have an active subscription to WSO2 products and a user to configure WUM. Also an internet connection is required by the WUM tool.After setting up WUM you can download the products by executing the following command. wum add wso2is-analytics-5.7.0We need to download the following set of products and you can repeat the above command with the following values. wso2is-km-5.7.0 wso2am-analytics-2.6.0 wso2am-2.6.0 wso2is-analytics-5.7.0After downloading you can list the products with the following command, wum listNext you can check whether wum updates are available for a given product. You can execute the following command to check for updates. wum check-update wso2ei-6.4.0This will give the following output if updates are available.Now you can do an update of the product. To do this execute the following command. wum update wso2ei-6.4.0You can repeat the above steps and get updates for all the products.After updating the product the product should be available at a location similar to following. /.wum3/products/wso2am/2.6.0/fullIf you perform multiple updates you will have multiple distributions in this location so make sure you use the distribution with the latest timestamp. Refer the following image.Preparing Openshift EnvironmentBefore we start deploying WSO2 artifact we need to prepare Openshift environment. In the openshift environment we need to create a project, service account etc. This needs to performed by the Openshift administrator. The Openshift administrator can refer the script 1-prepare-openshift.sh The content of this file is as following,#!/bin/bash # ------------------------------------------------------------------------ # Copyright 2019 WSO2, Inc. (http://wso2.com) # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License # ------------------------------------------------------------------------ oc=\\`which oc\\` # This is for creating the secret to authenticate with the Docker registry. DOCKER_REG_URL=\"docker.wso2.com\" PROJECT_NAME=$K8S_NAMESPACE DOCKER_REG_USERNAME=\"DOCKER_REG_PASSWORD\" DOCKER_REG_EMAIL=\"dev@wso2.com\" oc new-project $PROJECT_NAME --description=\"WSO2 Dev Project\" --display-name=\"WSO2 Dev\" oc project $PROJECT_NAME oc create serviceaccount wso2svc-account -n $PROJECT_NAME # Create the pvs # Most cases this will be created by the cluster admin, so DO NOT execute if already there. oc create -f ../k8s/volumes/persistent-volumes.yaml # Create the Rbac for WSO2 clustering oc create -f ../k8s/rbac/rbac.yaml # Creating the ADM policy to retrieve k8s cluster information from the svc account # We need to create a Docker registry secret if authentication is required by the registry. #oc create secret docker-registry wso2creds --docker-server=${DOCKER_REG_URL} --docker-username=${DOCKER_REG_USERNAME} --docker-password=${DOCKER_REG_PASSWORD} --docker-email=${DOCKER_REG_EMAIL}Creating the Docker images and PushingNote: You need to have docker installed in the system. Also unzip as well.After preparing the openshift environment and copying all the WSO2 distributions to a single directory we can start the docker image creation process. Note that I’m using the internal docker registry of Openshift here and you can modify these scripts and use any docker registry that is desired.Next navigate to the /scripts directory and open the 0-setEnvironment.sh and set the variables with appropriate values. The content of this file is as below and each variable will be explained below,Following variable name contains the project name/namespace of the WSO2 deployment, this name is specific to openshift and can be set depending on the environment.export K8S_NAMESPACE=\"wso2\"This is the docker registry URL of the Openshift environment, you can ask your openshift admin for this.export DOCKER_REGISTRY_URL=\"registry.apps.wso2.com\" export DOCKER_REGISTRY_NAMESPACE=$K8S_NAMESPACEThe name of the directory which contains the keystores that are being usedexport KEYSTORE_DIR_NAME=\"dev-env\" # The directory name which contains environment specific keystores in resources/keystores export WSO2_PACK_LOCATION=\"\" # The directory where WSO2 packs resideFollowing are the DNS names that are used for different WSO2 servers. Need to change the values depending on the environment.#DNS Names for the servers export APIM_HOST_NAME=\"apim.apps.wso2.com\" export APIM_GW_HOST_NAME=\"gw.apps.wso2.com\" export IS_HOST_NAME=\"identity.apps.wso2.com\"Following are the database details of WSO2#DB Details #For all the DB’s the same user will be used. export DB_URL=\"10.2.1.2:1433\" # host:port export DB_USER=\"root\" export DB_USER_PASSWORD=\"root123456\"WSO2 admin credentials.#Master admin of the WSO2 server export ADMIN_USER=\"admin\" export ADMIN_USER_PASSWORD=\"bpadmin\" export ADMIN_USER_PASSWORD_ENCODED=\"YnBhZG1pbg==\" # This is the base64 encoded value of the admin password, you can use [https://www.base64encode.org/](https://www.base64encode.org/) to encode.WSO2 keytstore credentials#Keystore Details export KEYSTORE_PASSWORD=\"wso2carbon\" export TRUSTORE_PASSWORD=\"wso2carbon\"After properly setting the variables source this file. Execute the following command to do this. source ./0-setEnvironment.shAfter sourcing the file execute the docker image creation script. ./2-createDockerImages.shThis will create the docker images. To push the docker images to the registry execute the following script. ./3-pushDockerImages.shAfter pushing the docker images you can browse the docker images in the Openshift console. You need to login to the openshift console and you need to have proper permissions to view the created project in Openshft. After login select the project and navigate to Builds -&gt; Images tab and the created images will be shown as below.Deploying APIMBefore deploying the products we need to bootstrap configurations, in order to do this execute the following command from within the scripts directory. ./4-bootstrap-configs.shNext we need to execute the apim deployment script. You can execute the following script from with the scripts directory. ./5-apim-is-deploy.shThe above will create the Openshift artifacts that are related to API Manager and IS. You can check the deployment process by navigating to the Openshift console. The progress of the deployment will be shown in the Openshift console.To check the status of the pods you can click on a deployment or you can open the pods page under Applications.Also from the pod you can check the events that are associated with the pod. This will reveal any errors that are generated in the deployment time.Also from the pods you can check the logs of the pods by navigating to the logs tab,If you need to access the pod itself you can navigate to the terminal tab and it will open a terminal session to the podAccessing the DeploymentOpenshift routes handle all inbound traffic that is received by the application. In-order to check the status of routes you can navigate to the routes section. The accessible URLs of the deployment will be shown in routes. You can navigate to a specified route to access the different components of APIM.So that’s it hope this helps and please drop a comment if you have any questions." }, { "title": "Parsing data between Jenkins Jobs", "url": "/posts/Parsing-data-between-Jenkins-Jobs/", "categories": "Devops, CICD, Jenkins", "tags": "jenkins, cicd, devops", "date": "2019-05-20 09:43:28 -0400", "snippet": "Jenkins is the most widely used CICD/Build tool that’s being used for many software projects. The stability and the extensibility of Jenkins has made it the number one go to tool when doing CICD work.In this post I will explain how you can parse some information from one build to the next build. My use case was, I was creating docker images with a build pipeline to update an environment and after updating a environment if something goes wrong in the environment the user has the ability to revert the environment. So inorder to rever the environment the build pipeline had to know the currently deployed versions.This could be simply done with some groovy code. In this case I was using Env Inject plugin and I updated the Environment variable with the information I needed for the next job and this env variable was read by the next Job. Following is the simple groovy code you can use for this.import org.jenkinsci.plugins.envinject.EnvInjectJobPropertyimport org.jenkinsci.plugins.envinject.EnvInjectJobPropertyInfonode('master') { script { echo 'Hello World' echo \"$JOB_NAME\" echo \"$perform\" persistInfo(); }}def persistInfo(){ def jenkins = Jenkins.instance def jobA = jenkins.getItemByFullName(\"$JOB_NAME\") def prop2 = jobA.getProperty(org.jenkinsci.plugins.envinject.EnvInjectJobProperty); def con = prop2.getInfo().getPropertiesContent(); def orig = [] def str = \"\" for(e in con.split(System.getProperty(\"line.separator\"))) { if (!e.contains(\"UPDATED_DOCKER_IMAGES\")) { str = str + e + System.getProperty(\"line.separator\") } } str = str + \"UPDATED_DOCKER_IMAGES=wso2apim-2.6.0\" def prop = new EnvInjectJobPropertyInfo(\"\", str , \"\", \"\", \"\", false) def propNew = new org.jenkinsci.plugins.envinject.EnvInjectJobProperty(prop) propNew.setOn(true) propNew.setKeepBuildVariables(true) propNew.setKeepJenkinsSystemVariables(true) jobA.addProperty(propNew) jobA.save();}What the above will do is read the existing environment variables and add a new environment variable called UPDATED_DOCKER_IMAGES. This property can be read from the next build.Hope this helps someone in need. Please drop a comment if you have any queries." }, { "title": "Generate Active AWS EC2 Instance Report", "url": "/posts/GenerateActiveAWSEC2InstanceReport/", "categories": "Devops, AWS", "tags": "java, aws, automation", "date": "2018-12-17 09:28:33 -0500", "snippet": "When sing AWS for running instance it’s important that we keep an eye on the running instances in-order to manage your cost. In most cases you will forget that you have started an instance to run your tests and It will run unnecessarily till you notice it. So in this pose I will explain how we can generate a simple Email everyday with the instances that are running. I will be using AWS SDK for this.Prerequisits : aws cli configured in your machine.Lets get started, So the final outcome will be something like below,I will be generating an html report As you can see I have highlighted the instances that are running more than 2 hours. Also it captures information like Intance Name, ID, State, Instance Type, Started time, Associated stack and uptime. So following is the simple code segment that I have used.package org.ycr.aws;import com.amazonaws.services.ec2.AmazonEC2;import com.amazonaws.services.ec2.AmazonEC2ClientBuilder;import com.amazonaws.services.ec2.model.DescribeInstancesRequest;import com.amazonaws.services.ec2.model.DescribeInstancesResult;import com.amazonaws.services.ec2.model.Instance;import com.amazonaws.services.ec2.model.Reservation;import com.amazonaws.services.ec2.model.Tag;import java.io.BufferedWriter;import java.io.File;import java.io.FileWriter;import java.io.IOException;import java.util.ArrayList;import java.util.Date;import java.util.List;import java.util.concurrent.TimeUnit;public class Main { public static void main(String[] args) { StringBuilder tempalte = new StringBuilder(); // Excludes any permanently running instances from the report List&lt;String&gt; excludeIdList = new ArrayList&lt;&gt;(); excludeIdList.add(\"i-0e637e411a26a3cd4\"); excludeIdList.add(\"i-0f978eabef81c5b20\"); tempalte.append(\"&lt;!DOCTYPE html&gt;\\n\" + \"&lt;html&gt;\\n\" + \"&lt;head&gt;\\n\" + \"&lt;/head&gt;\\n\" + \"&lt;body&gt;\\n\" + \"\\n\" + \"&lt;table id=\\\"customers\\\" border=\\\"1\\\" style=\\\"font-family: \\\"Trebuchet MS\\\", Arial, Helvetica, sans-serif; width: 100%;\\\"&gt;\\n\" + \" &lt;tr&gt;\\n\" + \" &lt;th&gt;Instance Name&lt;/th&gt;\\n\" + \" &lt;th&gt;Instance ID&lt;/th&gt;\\n\" + \" &lt;th&gt;State&lt;/th&gt;\\n\" + \" &lt;th&gt;Instance Type&lt;/th&gt;\\n\" + \" &lt;th&gt;Started Time Stamp&lt;/th&gt;\\n\" + \" &lt;th&gt;Associated Stack&lt;/th&gt;\\n\" + \" &lt;th&gt;Uptime(Hours)&lt;/th&gt;\\n\" + \" &lt;/tr&gt;\\n\"); final AmazonEC2 ec2 = AmazonEC2ClientBuilder.defaultClient(); boolean done = false; boolean runningInstancesAvailable = false; DescribeInstancesRequest request = new DescribeInstancesRequest(); while (!done) { DescribeInstancesResult response = ec2.describeInstances(request); for (Reservation reservation : response.getReservations()) { boolean exclude = false; for (Instance instance : reservation.getInstances()) { for (String id : excludeIdList) { if (id.equals(instance.getInstanceId())) { exclude = true; break; } } if (!exclude) { // Here I'm excluding instances that are t2.micro since they are included in the free tier. if (instance.getState().getName().equals(\"running\") &amp;&amp; !instance.getInstanceType().equals (\"t2.micro\")) { runningInstancesAvailable = true; // Get the insatance name String instanceName = \"Unknown\"; String stackName = \"No Stack!\"; // Iterating over the tags to find information ike stack name and instance name if (instance.getTags() != null) { for (Tag tag : instance.getTags()) { if (tag.getKey().equals(\"Name\")) { instanceName = tag.getValue(); System.out.println(\"Instance Name : \" + instanceName); } if (tag.getKey().equals(\"aws:cloudformation:stack-name\")){ stackName = tag.getValue(); } } } // Calculating the uptime long uptime = getDateDiff(instance.getLaunchTime(), new Date(), TimeUnit.HOURS); // If the uptime is more than 2 hours highlighting the text. if (uptime &gt; 2) { tempalte.append(\" &lt;tr style=\\\"color: #f71313; font-weight:bold\\\"&gt;\\n\" + \" &lt;td&gt;\" + instanceName + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getInstanceId() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getState().getName() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getInstanceType() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getLaunchTime().toString() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + stackName + \"&lt;/td&gt;\\n\" + \" &lt;td style=\\\"text-align:center\\\"&gt;\" + uptime + \"&lt;/td&gt;\\n\" + \" &lt;/tr&gt;\\n\"); } else { tempalte.append(\" &lt;tr&gt;\\n\" + \" &lt;td&gt;\" + instanceName + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getInstanceId() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getState().getName() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getInstanceType() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + instance.getLaunchTime().toString() + \"&lt;/td&gt;\\n\" + \" &lt;td&gt;\" + stackName + \"&lt;/td&gt;\\n\" + \" &lt;td style=\\\"text-align:center\\\"&gt;\" + uptime + \"&lt;/td&gt;\\n\" + \" &lt;/tr&gt;\\n\"); } System.out.printf( \"Found instance with id %s, \" + \"AMI %s, \" + \"type %s, \" + \"state %s \" + \"and monitoring state %s\", instance.getInstanceId(), instance.getImageId(), instance.getInstanceType(), instance.getState().getName(), instance.getMonitoring().getState()); } instance.getKeyName(); } } } request.setNextToken(response.getNextToken()); if (response.getNextToken() == null) { done = true; } } tempalte.append(\"&lt;/table&gt;\\n\" + \"&lt;/body&gt;\\n\" + \"&lt;/html&gt;\"); System.out.println(tempalte); if (runningInstancesAvailable){ File file = new File(\"./mail.html\"); try (BufferedWriter writer = new BufferedWriter(new FileWriter(file))) { writer.write(tempalte.toString()); writer.flush(); } catch (IOException e) { e.printStackTrace(); } } } /** * Get a diff between two dates * @param date1 the oldest date * @param date2 the newest date * @param timeUnit the unit in which you want the diff * @return the diff value, in the provided unit */ public static long getDateDiff(Date date1, Date date2, TimeUnit timeUnit) { long diffInMillies = date2.getTime() - date1.getTime(); return timeUnit.convert(diffInMillies,TimeUnit.MILLISECONDS); }}You can find the full project from here (https://github.com/yasassri/aws-observer). You can simply do an mvn clean install to generate the html content. Please drop a comment if you have any queries." }, { "title": "Create A Jenkins Job with a Groovy Script", "url": "/posts/CreateA-JenkinsJobwithaGroovyScript/", "categories": "CICD", "tags": "jenkins, cicd, devops, groovy, automation", "date": "2018-12-16 10:27:51 -0500", "snippet": "In this post I will share some useful groovy code that can be used when creating a Jenkins job using a groovy script, so this groovy script can be used within a declarative pipeline as well if you wish to create Jobs dynamically from within an another job. The simple code is as follows, I have added few inline comments so the code snippet will be easy to understand./** * This method is responsible for creating the Jenkins job. * @param jobName jobName * @param timerConfig cron expression to schedule the job * @return */def createJenkinsJob(def jobName, def timerConfig) { echo \"Creating the job ${jobName}\" // Here I'm using a shared library in the pipeline, so I have loaded my shared library here // You can simply have the entire pipeline syntax here. def jobDSL=\"@Library('yasassri@master') _\\n\" + \"Pipeline()\" def flowDefinition = new org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition(jobDSL, true) def instance = Jenkins.instance def job = new org.jenkinsci.plugins.workflow.job.WorkflowJob(instance, jobName ) job.definition = flowDefinition job.setConcurrentBuild(false) // Adding a cron configurations if cron configs are provided if (timerConfig != null &amp;&amp; timerConfig != \"\") { hudson.triggers.TimerTrigger newCron = new hudson.triggers.TimerTrigger(timerConfig); newCron.start(job, true) job.addTrigger(newCron) } // Here I'm adding a Job property to the Job, i'm using environment inject plugin here def rawYamlLocation = \"http://localhost:80/text.yaml\" def prop = new EnvInjectJobPropertyInfo(\"\", \"KEY=${rawYamlLocation}\", \"\", \"\", \"\", false) def prop2 = new org.jenkinsci.plugins.envinject.EnvInjectJobProperty(prop) prop2.setOn(true) prop2.setKeepBuildVariables(true) prop2.setKeepJenkinsSystemVariables(true) job.addProperty(prop2) job.save() Jenkins.instance.reload()}Hope the above was useful, please drop a comment if you have any concerns." }, { "title": "Testerina — Power of Mocking Revealed", "url": "/posts/TesterinaPowerofMockingRevealed/", "categories": "WSO2, Ballerina", "tags": "ballerina, wso2, testing", "date": "2018-06-04 05:09:31 -0400", "snippet": "Testerina is the built-in test framework for Ballerina. If anyone is wondering what is ballerina? Ballerina is a general purpose, concurrent, strongly typed and cloud native programming language which focuses on integrating and orchestrating microservices etc. If you want to learn more about ballerina you should read the rich content available at ballerina.ioThis post will explain the mocking capabilities testerina provides, which allows you to write independent self-contained tests for your application.Why mock?Why should we mock in the first place? Following are some cases where mocking is required. Too many integration points.A decade ago applications were linear, small, self contained and isolated. We used to have standalone applications which ran by it self, but not anymore. Todays applications are distributed and connected with many other integral components and endpoints. So when it comes to testing these connected application it’s not practically possible to test with all the real connections. So we need to mock these endpoints when we are testing these applications.2. Demand for higher rate of delivery.In good old days we had SDLC models like waterfall model, which is a lengthy process and releases only happened after months of development and testing. But nowadays we have daily releases, with CICD coming into the picture, build time of an application does matter a-lot when an application is to be delivered in quick pace. If the build takes days this would affect the release cycles and developers efficiency. So if we are running our tests with real component integrations which sometimes consumes more time (e.g : database integration) the release process will be drags to test execution time. So in-order to cutdown the test execution time we can mock the components that consumes more time.3. Test driven developmentTest driven development is one of the most popular trends in software development today. In test driven development developers writeup their tests first and keep on developing the feature till all the tests are passing. So what if the component you are building need to interact with a different component which is developed by a different team, can you wait for that feature to be completed? So again we might have to mock that component and do the development.Having explained why mocking is needed, lets see how testerina facilitates mocking. Testerina has two types of mocking, Function mocking. Service mocking.Function mockingTesterina provides the functionality to mock a function in a different third-party package with your own Ballerina function which will help you to test your package independently. Following is a code sample which uses function mocks.This feature is very powerful and can even be used to mock inbuilt functions, but this should be used with care. Following is an example where I have mocked the inbuilt println function.Service MockingCreate a mock service with ballerina service capabilitiesTesterina allows you to use existing ballerina service capabilities and start a mock services when a test requires a backend mock service. Following example shows how this can be done.In the above sample, for our tests to work we are starting a mock service called HelloServiceMock. Service startup is triggered in the before test function with startServices functionCreate a service mock with a swagger definitionIf you have a swagger definition, you can use that to create a mock service for your tests. Following is how you can generate a mock service out from a swagger definition.In the above sample in the init function we are calling the startServiceSkelaton method with the yaml definition which will start the service with the given name.This wraps-up this post, please feel free to drop a comment if you have any queries." }, { "title": "Customize and Manage WSO2 API-Manager Access Logs", "url": "/posts/CustomizeAndManageWSO2APIManagerAccessLogs/", "categories": "WSO2, API Manager", "tags": "wso2, wso2apim, tomcat, devops", "date": "2018-06-01 05:38:30 -0400", "snippet": "In any enterprise application it is important that we properly manage access logs which can be analyzed realtime or at a later stage. When it comes to WSO2 products they do provide access-login capabilities out of the box. But in some cases if you are using a tool to analyze the logs (e.g: splunk etc.) you will need the logs in a custom format. For example as key-value pares. So this POST explained how you can customize default logging pattern of WSO2 access logs.In WSO2 products like WSO2 ESB, EI, APIM there are two types of access, admin operation access and API/Service invocations. If you are familiar with WSO2 products it contains 2 sets of ports, 1 for Servlets Transport (9443/9763) and one for Passthrough (8280/8243). Servlet transport is where you get admin operation requests. By default all the access logs, coming into both servlet transport and passthrough transport are written to a common access log file located in repository/logs directory. But this default behavior can be changed and you can configure the logs to be written into two different files.Access logs for requests coming into the Servlet transport is handled by the logging mechanism of the embedded tomcat server. In-order to customize the access logs of the requests which comes into the servlet transport you can refer this guide.For generating access logs for requests coming into Passthrough transport (which are mostly API/Service invocations) WSO2 uses a separate implementation where most of the code is taken from the Tomcats logging feature it-self, but the attribute set it supports differ from tomcats’. Following is how you can customize API invocation logs of the APIM nodes. First lets create access log config file. By default this file is not available in API Manager. So lets create a file named access-log.properties in _/repository/conf_ Add the following content to this file. (All the supported options are in the following file, uncomment them to enable as required)# Default access log pattern#access_log_pattern=%{X-Forwarded-For}i %h %l %u %t \\“%r\\” %s %b \\”%{Referer}i\\” \\”%{User-Agent}i\\”# combinded log pattern#access_log_pattern=%h %l %u %t \\“%r\\” %s %b \\”%{Referer}i\\” \\”%{User-Agent}i\\”access_log_pattern=time=%t remoteHostname=%h localPort=%p localIP=%A requestMethod=%m requestURL=%U remoteIP=%a requestProtocol=%H HTTPStatusCode=%s queryString=%q# common log pattern#access_log_pattern=%h %l %u %t \\“%r\\” %s %b# file prefixaccess_log_prefix=http_gw# file suffixaccess_log_suffix=.log# file date formataccess_log_file_date_format=yyyy-MM-dd#access_log_directory=”/logs” Now restart the server and invoke an API.A new file will be created in the specified logging directory with the specified prefix. By default it will be created in _/repository/logs_ directory. Also note that this file support rolling file appending.Following are the attributes that are supported and these can be used in your login pattern to get request response information.%a - Remote IP address%A - Local IP address%b - Bytes sent, excluding HTTP headers, or ‘-‘ if zero%B - Bytes sent, excluding HTTP headers%c - Cookie value%C - Accept header%e - Accept Encoding%E - Transfer Encoding%h - Remote host name (or IP address if enableLookups for the connector is false)%l - Remote logical username from identd (always returns ‘-‘)%L - Accept Language%k - Keep Alive%m - Request method (GET, POST, etc.)%n - Content Encoding%r - Request Element%s - HTTP status code of the response%S - Accept Chatset%t - Date and time, in Common Log Format%T - Content Type%u - Remote user that was authenticated (if any), else ‘-‘%U - Requested URL path%v - Local server name%V - Vary Header%x - Connection Header%Z - Server HeaderHope this was helpful, please drop a comment if you have further queries!" }, { "title": "Perfect Your Ballerina Act — With Testerina", "url": "/posts/PerfectYourBallerinaActWithTesterina/", "categories": "WSO2, Ballerina", "tags": "ballerina, wso2, testing", "date": "2018-05-01 00:24:02 -0400", "snippet": "Designing a Testable ApplicationThis post is not about dancing!! Rather it’s about developing a microservice based testable end-to-end application with Ballerina and making sure it runs perfectly without any glitches using Testerina.Let me explain the unfamiliar terms! What is Ballerina? Ballerina is a general purpose, concurrent, strongly typed and cloud native programming language which focuses on integrating and orchestrating microservices etc. Testerina is the native testing framework built into Ballerina which enables Ballerina developers to capitalize on test driven development. It’s similar to TestNG for JAVA, Go-test for Go-lang etc.Along the Test Pyramid,Before we go into details let’s look at some well-known testing strategies. The test pyramid is one popular strategy we can adopt when designing tests. Test pyramid illustrates the composition of tests which is ideal to have at different testing layers. Following image gives an idea about the test pyramid.It’s notable that when you move up the pyramid the execution time for tests and the component size of the application may increase. So as a rule of thumb it’s ideal to have more unit tests and less end to end tests. It doesn’t mean you shouldn’t have end-to-end tests at all, we should find the correct mix of tests ideal for your application. Lets dive a little deeper and understand each layer better.Unit Testing.An unit represents an isolated code fragment that can be tested without any external dependencies. For example, A function which adds two numbers and return the sum.Integration Testing.Integration tests are done when multiple components integrate with each other. For e.g: Persistence layer interacting with the DB, Service layer interacting with the service implementations etc.End to End Testing.End to end testing layer depicts, testing a production equivalent deployment of the entire solution. The end to end testing deployment will mimic the actual production deployment and perform tests on top of this environment.Design decisions; Making your application *Testable*In-order to efficiently test your application it should be designed in a testable manner. This involves a set of conscious design decision a developer should take to make sure the application is testable, not as a monolithic app but as a set of units which aligns with the test pyramid strategy. And the approach you follow to make your application testable may vary from application to application. In this post, I will use a sample based on microservices, to explain how you can design a testable application with Ballerina and test it with Testerina.Theatre; A stage to demonstrateLets come up with a sample application. Theatre Management App; An event and ticket management application. This will explain how an end to end application can be designed based on microservices.I have modeled the sample application as a collection of microservices which orchestrates the functional flow of the use-cases. Theatre app contains 3 microservices (Portal, TicketingMS, EventHandlerMS). Following image depicts the high level architecture of the sample. Please note that Payment Gateway is an external service which only acts as an Endpoint.Portal (Composite Application)Portal acts as the composite service which does the plumbing between other microservices and the external service. (Ticketing, Event and Payment Gateway). Only the portal app will be exposed as the public user interface and it will route and mediate the incoming messages.Event Handler MSThis service is responsible for handling events, registering events, retrieving event information etc.Ticketing MSTicketing service is responsible for handling ticketing related operations. Viewing available Tickets, purchasing tickets etc.Theatre; Message FlowsLet’s look at the main message flows of the Theatre application. Mainly the Event organizers can register their event with available ticket information. Then the users can browse through the events and purchase tickets by providing credit card information.Register Event Message flowFollowing sequence diagram depicts the event registration message flow. The user will be sending event and ticket information in a single request which will be extracted by portal service and sent to other microservices.As you can see, the register message is routed through different services and the flow is orchestrated by the Portal service.Purchase Ticket Message flowOnce an event is added, the users can browse the events and purchase the tickets. Following sequence diagram depicts the ticket purchasing message flow.Breaking the monolithicityIf we look at the L0 architecture of the solution, It’s quite large to be tested as a whole. It interacts with external services which you may not have any control over. And also keep in mind the Test Pyramid, you should always minimize testing at end-to-end layer.We can break the entire solution into following testable isolated components.Each separated component can be tested isolated, by mocking the interactions with each other.Microservice ArchitectureIf we further dive into the actual microservice, I have componetise the microservices which will allow me to separate and isolate functionality. Following is the microservices component architecture each service is made of. This architecture may vary from application to application. Services/Resources : The actual service interfaces exposed to the users, where requests are accepted and taken in. Service Implementations : Service layer dispatches the request information to service Implementation layer where the payload get processed. Models : Ballerina objects to store your data. e.g : Register Event Request is represented as an object, when the request arrives, an object (A Model) is created with request information, which is parsed around the program. Utility Functions : Functions that act as helper functions, used for different operations. e.g : Generate a json error from a string etc. Persistence : Persistence functions takes care of DB interactions. Connectors : Connectors are responsible for calling external services and other micro services.Messages flow through a microservice as illustrated in the following diagram.As you can see above, the message gets strip down and validated along the execution path. The response path is also similar to the above.Identifying the testable unitsSo lets see how we can identify testable components from the sample app. If we look into individual microservices closely, following is how a microservice can be further dissected into testable components. The basis for selecting components is the functional independence of each component.Let me explain the reasons behind drawing the above boundaries around different components.Models Test BoundaryModels are the smallest entities that can be tested, and the accessibility of model elements can be tested here so It’s a self contained testable component.Utility Functions Test BoundaryUtility functions can also be tested isolated, since most of them do not depend on any other functions. At this level we can test the individual utility functions extensively with less distractions.Service Impl Testing BoundaryThen if we consider service implementation boundary, It contains models, utility functions and the service implementation which needs to be tested. We can try to isolate the service implementation layer from models and utility functions but it will not give an advantage, since mocking the utility functions may needs more effort than using the actual function it self. Another thing to note is the best way to test your function is without mocking interactions since it reflects the actual interaction then. But again theBut the persistence layer should be decoupled from the service impl layer since it requires a database to connect, hence we should introduce a mocking layer between the service implementation layer and persistence layer.Service Testing boundaryService testing boundary includes Service Imple, utility functions and models. Since interaction between these components are not heavy, we can include the actual components without mocking them. Again we need to mock the persistence layer and the connectors to test the service layer.Connectors test boundaryConnectors also can be tested isolated by mocking the external interactions.Persistence test boundaryWe can test the persistence layer by mocking the Database interactions.So that wraps up this post. Please drop a comment if you have thought or queries. Next post will explain how the actual app looks like and how we can write tests for our application with Testerina." } ]
